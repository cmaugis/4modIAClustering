[
  {
    "objectID": "TP3-CAH-SujetEtudiant.html",
    "href": "TP3-CAH-SujetEtudiant.html",
    "title": "TP 3 : CAH",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées pour classification hiérarchique. Les librairies R nécessaires pour ce TP :\nlibrary(mclust)\nlibrary(clusterSim)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(circlize)\nlibrary(viridis)"
  },
  {
    "objectID": "TP3-CAH-SujetEtudiant.html#lecture-des-données-et-fonctions-auxiliaires",
    "href": "TP3-CAH-SujetEtudiant.html#lecture-des-données-et-fonctions-auxiliaires",
    "title": "TP 3 : CAH",
    "section": "2.1 Lecture des données et fonctions auxiliaires",
    "text": "2.1 Lecture des données et fonctions auxiliaires\nOn reprend dans cette partie le jeu de données zoo ainsi que les fonctions auxiliaires comme dans le TP1. On refait une analyse en composantes multiples pour la suite.\n\nzoo&lt;-read.table(\"zoo-dataTP.txt\",header=T,stringsAsFactors = TRUE)\nfor (j in 1:ncol(zoo))\n  zoo[,j]&lt;-as.factor(zoo[,j])\nsummary(zoo)\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\nres.mca&lt;- MCA(zoo,ncp = 5, graph = FALSE)\n\nfviz_screeplot(res.mca)\nplot(res.mca, invisible = c(\"quali.sup\", \"ind\"), cex=1, col.var = \"darkblue\", cex.main=2, col.main= \"darkblue\")\n\nfviz_mca_ind(res.mca)\n\nPour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction barplotClus() permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d’un clustering donné.\n\n# J indice des variables\n# Data = jeu de données\n# clust = clustering étudié\n# output : liste des graphes par variable dans J donnant la répartition des modalités de J par classe de clust\n\nbarplotClus &lt;- function(clust, Data, J) {\n    aux.long.p &lt;- heatm(clust, Data, J)$freq\n    p&lt;-NULL\n    \n    for (j in 1:length(J)) {\n        p[[j]] &lt;- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[J[j]]), ],\n            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = \"identity\")+\n            labs(fill = colnames(Data)[J[j]])\n    }\n    return(p)\n}\nheatm &lt;- function(clust, Data, J) {\n    library(dplyr)\n    Dataaux &lt;- data.frame(id.s = c(1:nrow(Data)), Data)\n    aux &lt;- cbind(Dataaux, clust)\n    aux.long &lt;- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,\n        id = c(\"id.s\", \"clust\"), factorsAsStrings = T)\n    # Effectifs\n    aux.long.q &lt;- aux.long %&gt;%\n        group_by(clust, variable, value) %&gt;%\n        mutate(count = n_distinct(id.s)) %&gt;%\n        distinct(clust, variable, value, count)\n    # avec fréquences\n    aux.long.p &lt;- aux.long.q %&gt;%\n        group_by(clust, variable) %&gt;%\n        mutate(perc = count/sum(count)) %&gt;%\n        arrange(clust)\n\n    Lev &lt;- NULL\n    for (j in 1:ncol(Data)) Lev &lt;- c(Lev, levels(Data[, j]))\n\n    Jaux &lt;- NULL\n    for (j in 1:length(J)) {\n        Jaux &lt;- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))\n    }\n\n    gaux &lt;- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +\n        scale_fill_gradient2(low = \"white\", mid = \"blue\", high = \"red\") + theme_minimal()\n\n    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))\n}"
  },
  {
    "objectID": "TP3-CAH-SujetEtudiant.html#cah-directe-sur-variables-qualitatives",
    "href": "TP3-CAH-SujetEtudiant.html#cah-directe-sur-variables-qualitatives",
    "title": "TP 3 : CAH",
    "section": "2.2 CAH directe sur variables qualitatives",
    "text": "2.2 CAH directe sur variables qualitatives\nQuestion : Quelle classification par CAH pouvez-vous mettre en place pour traiter des données qualitatives ? Mettez en application votre proposition avec R et étudiez la classification retenue. Vous pourrez vous aider des fonctions daisy() de la librairie cluster, de la fonction hclust(), …\n\n# A COMPLETER"
  },
  {
    "objectID": "TP3-CAH-SujetEtudiant.html#cah-sur-les-coordonnées-de-mca",
    "href": "TP3-CAH-SujetEtudiant.html#cah-sur-les-coordonnées-de-mca",
    "title": "TP 3 : CAH",
    "section": "2.3 CAH sur les coordonnées de MCA",
    "text": "2.3 CAH sur les coordonnées de MCA\nQuestion : Mettez en place une classification des animaux à partir des coordonnées de l’analyse en composantes multiples. Comparez avec la classification obtenue dans la section précédente.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP2-DBSCAN-SujetEtudiant.html",
    "href": "TP2-DBSCAN-SujetEtudiant.html",
    "title": "TP 2 : DBSCAN",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées pour la méthode DBSCAN. Les librairies R nécessaires pour ce TP :\nlibrary(mclust)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(dbscan)\nlibrary(seriation)"
  },
  {
    "objectID": "TP2-DBSCAN-SujetEtudiant.html#reprise-des-données",
    "href": "TP2-DBSCAN-SujetEtudiant.html#reprise-des-données",
    "title": "TP 2 : DBSCAN",
    "section": "1.1 Reprise des données",
    "text": "1.1 Reprise des données\nOn reprend dans ce TP les données wine disponibles sur la page moodle du cours. On charge ici les données.\n\nwine&lt;-read.table(\"wine.txt\",header=T)\nwine$Qualite = as.factor(wine$Qualite)\nwine$Type = factor(wine$Type, labels = c(\"blanc\", \"rouge\"))\n\nwineinit&lt;-wine\nwine[,-c(1,2)]&lt;-scale(wine[,-c(1,2)],center=T,scale=T)\n\nhead(wine)\n\nOn fait une ACP pour la visualisation des résultats dans la suite\n\nresacp&lt;-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)\nfviz_pca_ind(resacp,habillage=2,geom=c(\"point\"))"
  },
  {
    "objectID": "TP2-DBSCAN-SujetEtudiant.html#dbscan-à-paramètres-fixés",
    "href": "TP2-DBSCAN-SujetEtudiant.html#dbscan-à-paramètres-fixés",
    "title": "TP 2 : DBSCAN",
    "section": "1.2 DBSCAN à paramètres fixés",
    "text": "1.2 DBSCAN à paramètres fixés\nQuestion : Dans un premier temps, utilisez l’algorithme DBSCAN avec les paramètres minPts= 7 et eps= 1 à l’aide de la fonction dbscan() de la librairie dbscan. Quels sont les effectifs par classe ? Combien d’individus ne sont pas classés ?\n\n# A COMPLETER\nminPts&lt;-7\neps&lt;-1\nres.db &lt;- dbscan::dbscan(...)\ntable(...)\n\n\nfviz_cluster(res.db, wine[,-c(1:2)], geom=\"point\",ellipse=\"FALSE\")+\n  theme(legend.position=\"none\")+\n  xlab(\"\")+ylab(\"\")+ggtitle(\"Avec DBSCAN\")"
  },
  {
    "objectID": "TP2-DBSCAN-SujetEtudiant.html#influence-des-paramètres-de-dbscan",
    "href": "TP2-DBSCAN-SujetEtudiant.html#influence-des-paramètres-de-dbscan",
    "title": "TP 2 : DBSCAN",
    "section": "1.3 Influence des paramètres de DBSCAN",
    "text": "1.3 Influence des paramètres de DBSCAN\nQuestion : Pour étudier l’influence des paramètres minPts et eps, évaluez le nombre de classes obtenues et le nombre d’individus non classés pour différentes valeurs de ces paramètres.\n\nminPts &lt;- ...\neps &lt;- ...\nNBCluster &lt;- matrix(0,nrow=length(minPts),ncol=length(eps))\nNBNonCl &lt;-matrix(0,nrow=length(minPts),ncol=length(eps))\nfor (i in 1:length(minPts)){\n  for (j in 1:length(eps)){\n    res&lt;-dbscan::dbscan(wine[,-c(1,2)], eps=eps[j], minPts=minPts[i])\n    NBCluster[i,j] &lt;- ...\n    NBNonCl[i,j] &lt;- ...\n  }\n}\n\ndf&lt;-data.frame(eps=rep(eps,each=length(minPts)),\n              minPts=as.factor(rep(minPts,length(eps))),\n              NBCluster=c(NBCluster),\n              NBNonCl=c(NBNonCl)*100/nrow(wine))\n\nggplot(df,aes(x=eps,y=NBCluster,col=minPts))+geom_point()+geom_line()\nggplot(df,aes(x=eps,y=NBNonCl,col=minPts))+geom_point()+geom_line()\n\nQuestion : Pour une valeur de minPts=7, tracez le graphe de distance kNN afin de choisir le paramètre eps. Vous pouvez utiliser la fonction kNNdistplot(). Qu’en pensez-vous ?\n\n# A COMPLETER"
  },
  {
    "objectID": "TP2-DBSCAN-SujetEtudiant.html#comparaison-avec-les-kmeans",
    "href": "TP2-DBSCAN-SujetEtudiant.html#comparaison-avec-les-kmeans",
    "title": "TP 2 : DBSCAN",
    "section": "1.4 Comparaison avec les Kmeans",
    "text": "1.4 Comparaison avec les Kmeans\nQuestion : A l’aide des questions précédentes, choisissez des paramètres pour obtenir un clustering à 4 classes. Comparez cette classification avec celle obtenue par les Kmeans pour le même nombre de classes.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html",
    "href": "TP1-Kmeans-SujetEtudiant.html",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées dans le chapitre dédié aux algorithmes de clustering de type Kmeans. Les librairies R nécessaires pour ce TP :\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(mclust)\nlibrary(cluster)\nlibrary(ppclust)\nlibrary(ggalluvial)\nlibrary(klaR)\nlibrary(gridExtra)\nlibrary(reshape2)"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#analyse-descriptive-des-données",
    "href": "TP1-Kmeans-SujetEtudiant.html#analyse-descriptive-des-données",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "1.1 Analyse descriptive des données",
    "text": "1.1 Analyse descriptive des données\n\n1.1.1 Présentation des données de vins\nDans ce TP, on va utiliser le jeu de données wine disponible sur la page moodle du cours.\nCe jeu de données comprend des mesures physico-chimiques réalisées sur un échantillon de \\(n=600\\) vins (rouges et blancs) du Portugal. Ces mesures sont complétées par une évaluation sensorielle de la qualité par un ensemble d’experts. Chaque vin est décrit par les variables suivantes :\n\nQualite : son évaluation sensorielle par les experts (“bad”,“medium”,“good”),\nType : son type (1 pour un vin rouge, 0 pour un vin blanc),\nAcidVol : la teneur en acide volatile (en g/dm3 d’acide acétique),\nAcidCitr : la teneur en acide citrique (en g/dm3),\nSO2lbr : le dosage du dioxyde de soufre libre (en mg/dm3),\nSO2tot : le dosage du dioxyde de soufre total (en mg/dm3),\nDensite : la densité (en g/cm3),\nAlcool : le degré d’alcool (en % Vol.).\n\nQuestion Récupérez sur moodle le jeu de données wine.txt et chargez-le sous R.\n\nwine &lt;-read.table(.......)\n\nVérifiez la nature des variables à l’aide de la fonction str(). Modifiez si nécessaire les variables qualitatives (à l’aide de as.factor()) et transformez les modalités “1” et “0” de la variable Typeen “rouge” et “blanc” respectivement (à l’aide de la fonction factor()).\n\nwine$Qualite &lt;- ....\nwine$Type &lt;- factor(....)\n\n\n\n1.1.2 Statistiques descriptives\nQuestion Faites quelques statistiques descriptives pour faire connaissance avec le jeu de données, avec des choix adaptés à la nature des variables. En particulier, étudiez les corrélations entre les variables quantitatives et faites une ACP.\n\n# A completer\n\nQuestion : Pour la suite, on va utiliser les variables quantitatives pour faire de la classification non supervisée des vins. Les variables Qualite et Type seront utilisées comme des variables extérieures pour comparer / croiser avec les classifications obtenues pour l’interprétation.\nPensez-vous qu’il est nécessaire de transformer les variables quantitatives dans l’objectif de clustering avec un algorithme des Kmeans ? Si oui, mettez en place cette transformation.\n\n# A completer"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-des-kmeans",
    "href": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-des-kmeans",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "1.2 Classification avec l’algorithme des Kmeans",
    "text": "1.2 Classification avec l’algorithme des Kmeans\n\n1.2.1 A K=3 fixé\nQuestion : A l’aide de la fonction kmeans(), faites une classification non supervisée en 3 classes des vins. Regardez les options disponibles dans la fonction kmeans().\n\nhelp(kmeans)\nreskmeans&lt;-kmeans(....)\n\nQuestion : Combien a-ton de vins par classe ? Visualisez la classification obtenue dans les premiers plans de l’ACP (vous pouvez utiliser la fonction PCA() de la librairie FactoMineR et la fonction fviz_cluster de la librairie factoextra).\n\n# A COMPLETER\n\nQuestion : La classification obtenue précédemment a-t-elle un lien avec le type de vins ? Avec la qualité du vin ? Vous pouvez vous aider de la fonction table(), la fonction adjustedRandIndex() de la librairie mclust, …\n\n# A COMPLETER\n\n\n\n1.2.2 Choix du nombre de classes\nQuestion : On s’intéresse dans cette section au choix du nombre de classes \\(K\\) en étudiant l’évolution de l’inertie intraclasse. En faisant varier \\(K\\) entre 2 et 15, calculez l’inertie intraclasse associée à chaque classification obtenue. Tracez l’évolution de l’inertie intraclasse en fonction du nombre de classes. Qu’en concluez-vous ?\n\n# A completer\nKmax&lt;-15\nreskmeanscl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nIintra&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-kmeans(...)\n  reskmeanscl[,k-1]&lt;-resaux$...\n  Iintra&lt;-c(Iintra,resaux$...)\n}\n\ndf&lt;-data.frame(K=2:15,Iintra=Iintra)\nggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()+xlab(\"Nombre de classes\")+ylab(\"Inertie intraclasse\")\n\nQuestion : Reprendre la question du choix du nombre de classes en utilisant le critère silhouette (vous pouvez vous aider de la fonction silhouette() de la librairie cluster). Pour la classification sélectionnée, représentez les poids \\(s(i)\\) de chaque individu à l’aide de la fonction fviz_silhouette().\n\n# A COMPLETER\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n   aux&lt;-silhouette(..., daisy(wine[,-c(1,2)]))\n   Silhou&lt;-c(Silhou,mean(aux[,3]))\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\naux&lt;-silhouette(...)\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\nrm(df,Silhou,aux)"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-pam",
    "href": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-pam",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "1.3 Classification avec l’algorithme PAM",
    "text": "1.3 Classification avec l’algorithme PAM\nQuestion : Déterminez une classification en \\(K=3\\) classes des vins en utilisant la méthode PAM (fonction pam()de la librairie cluster) et représentez graphiquement la classification obtenue. A-t-elle un lien avec le type de vins ? Avec la qualité ? Avec la classification en \\(K=3\\) classes obtenue avec la méthode des Kmeans?\n\n# A COMPLETER\n\nresPAM&lt;-pam(x=...,k=...,metric=...)\nresPAM$medoids\nresPAM$id.med\n\nfviz_cluster(resPAM,data=wine[,-c(1,2)],ellipse.type=\"norm\",labelsize=8,geom=c(\"point\"))+ggtitle(\"\")\nfviz_pca_ind(resacp,col.ind=as.factor(resPAM$clustering),geom = c(\"point\"),axes=c(1,2))\n\nadjustedRandIndex(..., ...)\ntable(..., ....)\n\nclust1&lt;-paste(\"K-k\",......,sep=\"\")\nclust2&lt;-paste(\"PAM-k\",......,sep=\"\")\nTab&lt;-melt(table(clust1,clust2))\nggplot(Tab,aes(y=value,axis1=clust1,axis2=clust2))+\n  geom_alluvium(aes(fill=clust1))+\n  geom_stratum(width = 1/12)+   \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  theme(legend.position = \"none\")\n\nQuestion : Déterminez le nombre de classes optimal par le critère Silhouette pour \\(K\\) variant entre 2 et 15 avec l’algorithme PAM. Commentez la classification retenue. Est-elle proche de celle obtenue avec l’algorithme des Kmeans ?\n\n# A completer\n\nKmax&lt;-15\nresPAMcl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-pam(.....)\n  resPAMcl[,k-1]&lt;-resaux$clustering\n  aux&lt;-silhouette(resPAMcl[,k-1], daisy(wine[,-c(1,2)]))\n  Silhou&lt;-c(Silhou,......)\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\n\naux&lt;-silhouette(......, daisy(wine[,-c(1:2)]))\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\n\nadjustedRandIndex(.....)\ntable(.....)"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-fuzzy-c-means",
    "href": "TP1-Kmeans-SujetEtudiant.html#classification-avec-lalgorithme-fuzzy-c-means",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "1.4 Classification avec l’algorithme fuzzy c-means",
    "text": "1.4 Classification avec l’algorithme fuzzy c-means\n\n1.4.1 Présentation\nAvec les algorithmes de clustering précédents (Kmeans, PAM) nous obtenons une classification “dure” au sens que chaque individu ne peut appartenir qu’à une seule classe et chaque individu participe avec le même poids à la construction des classes. Une classification dure \\(\\mathcal{P}_K=\\{\\mathcal{C}_1,\\ldots,\\mathcal{C}_K\\}\\) peut se traduire en une matrice \\(Z=(z_{ik})_{\\underset{1\\leq k \\leq K}{1\\leq i \\leq n}}\\) avec \\(z_{ik}=1\\) si \\(i\\in\\mathcal{C}_k\\) et 0 sinon. Dans cette section, nous allons nous intéresser à une adaptation de l’algorithme des Kmeans, appelée fuzzy c-means. L’idée est de retourner une classification fuzzy c’est-à-dire une matrice \\(W=(\\omega_{ik})_{\\underset{1\\leq k \\leq K}{1\\leq i \\leq n}}\\) avec \\(\\forall i,\\ k,\\ \\omega_{ik}\\geq 0\\) et \\(\\forall i,\\ \\underset{k=1}{\\stackrel{K}{\\sum}} \\omega_{ik}=1\\). On donne ainsi plutôt un poids \\(\\omega_{ik}\\) que l’individu \\(i\\) appartienne à la classe \\(\\mathcal{C}_k\\).\nL’algorithme fuzzy c-means a pour fonction objective\n\\[\n\\underset{W,\\{m_1,\\ldots,m_K\\}}{\\mbox{argmin}}\\ \\underset{i=1}{\\stackrel{n}{\\sum}}\\underset{k=1}{\\stackrel{K}{\\sum}} (\\omega_{ik})^\\gamma\\ \\|x_i - m_k\\|^2\n\\] où \\(X=(x_1,\\ldots,x_n)'\\) est la matrice des données, \\(\\gamma\\in[1,+\\infty[\\), \\(m_k\\) est le centre de la classe \\(\\mathcal{C}_k\\).\nDans le même principe que l’algorithme des Kmeans, l’algorithme fuzzy c-means est un algorithme itératif :\n\nStep 1: Initialisation des poids \\(W^{(0)}\\)\nStep 2: A l’itération \\(r\\), on calcule les centres des classes\n\n\\[\nm_k^{(r)} = \\frac{\\underset{i=1}{\\stackrel{n}{\\sum}} (\\omega_{ik}^{(r-1)})^\\gamma x_i}{\\underset{i=1}{\\stackrel{n}{\\sum}} (\\omega_{ik}^{(r-1)})^\\gamma}\n\\]\n\nStep 3: Mise à jour des poids (\\(\\gamma&gt;1\\)) \\[\n\\omega_{ik}^{(r)} = \\left[\\underset{\\ell=1}{\\stackrel{K}{\\sum}} \\left(\\frac{\\|x_i - m_k^{(r)}\\|^2}{\\|x_i - m_\\ell^{(r)}\\|^2}\\right)^{\\frac{1}{\\gamma-1}}  \\right]^{-1}\n\\]\nStep 4: Si \\(\\|W^{(r)} - W^{(r-1)}\\|&lt;\\) seuil, on s’arrête, sinon on retourne à l’étape 2.\n\nEn général, la puissance choisie sur les poids est \\(\\gamma=2\\). Dans le cas \\(\\gamma=1\\), on retrouve l’algorithme des Kmeans.\n\n\n1.4.2 Application avec R\nNous allons ici nous appuyer sur la fonction fcm() de la librairie ppclust.\nQuestion : Utilisez cet algorithme pour obtenir une classification en \\(3\\) classes. Comment sont initialisés les poids ? Comment est obtenue la classification finale ? A l’aide des poids, étudiez la stabilité des classes. Vous pouvez pour cela étudier les poids des individus par classe.\n\n# A COMPLETER\nlibrary(ppclust)\nresfcm&lt;-fcm(...)\n\nQuestion : Représentez la classification obtenue sur le premier plan de l’ACP en nuançant selon les poids.\n\n# A COMPLETER\nfviz_pca_ind(resacp,axes=c(1,2),geom=c(\"point\"),col.ind=apply(....................))+\nscale_color_gradient2(low=\"white\", mid=\"blue\",high=\"red\", midpoint=0.8, space = \"Lab\")\n\nQuestion : Comparez les classifications obtenues avec Kmeans et fuzzy c-means. Commentez.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#statistiques-descriptives-1",
    "href": "TP1-Kmeans-SujetEtudiant.html#statistiques-descriptives-1",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "2.1 Statistiques descriptives",
    "text": "2.1 Statistiques descriptives\nQuestion : Commencez par charger le jeu de données zoo-dataTP.txt et faites quelques statistiques descriptives pour vous familiariser avec ce jeu de données.\n\nzoo&lt;- ...\n# A COMPLETER\n\nQuestion : Faites une analyse en composantes multiples de ce jeu de données.\n\nres.mca&lt;-MCA(....)\n# A COMPLETER"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#fonctions-auxiliaires-pour-la-suite",
    "href": "TP1-Kmeans-SujetEtudiant.html#fonctions-auxiliaires-pour-la-suite",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "2.2 Fonctions auxiliaires pour la suite",
    "text": "2.2 Fonctions auxiliaires pour la suite\nPour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction barplotClus() permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d’un clustering donné.\n\n# J indice des variables\n# Data = jeu de données\n# clust = clustering étudié\n# output : liste des graphes par variable dans J donnant la répartition des modalités de J par classe de clust\n\nbarplotClus &lt;- function(clust, Data, J) {\n    aux.long.p &lt;- heatm(clust, Data, J)$freq\n    p&lt;-NULL\n    \n    for (j in 1:length(J)) {\n        p[[j]] &lt;- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[J[j]]), ],\n            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = \"identity\")+\n            labs(fill = colnames(Data)[J[j]])\n    }\n    return(p)\n}\nheatm &lt;- function(clust, Data, J) {\n    library(dplyr)\n    Dataaux &lt;- data.frame(id.s = c(1:nrow(Data)), Data)\n    aux &lt;- cbind(Dataaux, clust)\n    aux.long &lt;- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,\n        id = c(\"id.s\", \"clust\"), factorsAsStrings = T)\n    # Effectifs\n    aux.long.q &lt;- aux.long %&gt;%\n        group_by(clust, variable, value) %&gt;%\n        mutate(count = n_distinct(id.s)) %&gt;%\n        distinct(clust, variable, value, count)\n    # avec fréquences\n    aux.long.p &lt;- aux.long.q %&gt;%\n        group_by(clust, variable) %&gt;%\n        mutate(perc = count/sum(count)) %&gt;%\n        arrange(clust)\n\n    Lev &lt;- NULL\n    for (j in 1:ncol(Data)) Lev &lt;- c(Lev, levels(Data[, j]))\n\n    Jaux &lt;- NULL\n    for (j in 1:length(J)) {\n        Jaux &lt;- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))\n    }\n\n    gaux &lt;- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +\n        scale_fill_gradient2(low = \"white\", mid = \"blue\", high = \"red\") + theme_minimal()\n\n    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))\n}"
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#clustering-à-laide-des-kmodes",
    "href": "TP1-Kmeans-SujetEtudiant.html#clustering-à-laide-des-kmodes",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "2.3 Clustering à l’aide des Kmodes",
    "text": "2.3 Clustering à l’aide des Kmodes\nDans cette partie, nous allons utiliser la méthode des Kmodes introduite par Huang (1998). Rappelons que cette méthode est une extension des Kmeans dans le cas des données qualitatives. Les modifications par rapport aux Kmeans sont\n\nle changement de distance : on utilise la dissimilarité basée sur l’appariement simple\n\n\\[\nd(\\mathbf{x}_i,\\mathbf{x}_\\ell) = \\underset{j=1}{\\stackrel{p}{\\sum}}\\ \\mathbb{1}_{x_{ij}\\neq x_{\\ell j}}\n\\]\n\nle centre d’une classe est calculé en fonction des fréquences des modalités majoritaires présentes dans cette classe: pour la classe \\(\\mathcal{C}_k\\),\n\n\\[\n\\mathbf{m}_k=(m_{k1},\\ldots,m_{kp}) \\textrm{ avec } m_{kj}= \\underset{u_1,\\ldots,u_{s_j}}{\\mbox{argmax}}\\ \\underset{i\\in\\mathcal C_k}{\\sum}\\ \\mathbb{1}_{x_{ij}= u_{s_j}}\n\\]\nQuestion : A l’aide de la fonction kmodes()de la librairie klaR, déterminez une classification en \\(K=6\\) classes des animaux. Visualisez la classification obtenue. Vous pouvez vous aider des fonctions auxiliaires pour interpréter la classification.\n\nlibrary(klaR)\nreskmodes&lt;-kmodes(....)\n\nQuestion : Pour déterminer le nombre de classes, la méthode du coude peut être utilisée en remplaçant l’inertie intra-classe par le critère “Within Cluster Difference”\n\\[\nWCD(K) = \\underset{k=1}{\\stackrel{K}{\\sum}} \\underset{i\\in\\mathcal C_k}{\\sum}\\ d(x_i,m_k)  \n\\]\noù \\(d(.,.)\\) est l’appariement simple et \\(m_k\\) est le centre de la classe \\(\\mathcal C_k\\).\nTracez la courbe \\(K\\mapsto WCD(K)\\) pour déterminer le nombre de classes optimal. Vous pouvez vous aider des sorties de la fonction kmodes().\n\n# A COMPLETER\nWithinDiff&lt;-NULL\nKmax&lt;-10\nClust&lt;-matrix(0,nrow=nrow(zoo),ncol=Kmax)\nfor (k in 1:Kmax){\n  aux&lt;-kmodes(.........)\n  WithinDiff&lt;-c(WithinDiff,..........)\n  Clust[,k]&lt;-aux$cluster\n}\n\nauxdf&lt;-data.frame(NbCluster=1:Kmax,WithinDiff=WithinDiff)\nggplot(auxdf,aes(x=NbCluster,y=WithinDiff))+geom_point()+\n  geom_line()\n\nQuestion Etudiez la classification retenue. On la notera clustkmodes pour la suite\n\n# A COMPLETER\nclustkmodes&lt;- ..."
  },
  {
    "objectID": "TP1-Kmeans-SujetEtudiant.html#clustering-avec-les-kmeans-sur-les-coordonnées-de-acm",
    "href": "TP1-Kmeans-SujetEtudiant.html#clustering-avec-les-kmeans-sur-les-coordonnées-de-acm",
    "title": "TP 1 : Kmeans et ses variantes",
    "section": "2.4 Clustering avec les Kmeans sur les coordonnées de ACM",
    "text": "2.4 Clustering avec les Kmeans sur les coordonnées de ACM\nUne seconde stratégie est de partir des coordonnées de l’analyse des correspondances multiples (ACM) et d’utiliser un algorithme plus usuel sur données quantitatives. Dans cette section, on va appliquer l’algorithme des Kmeans.\nQuestion : Appliquez l’algorithme des Kmeans sur les coordonnées de l’ACM. Pour la détermination du nombre de classes, vous pouvez utiliser l’évolution de l’inertie intra-classe et le critère silhouette.\n\n# A COMPLETER\n\nQuestion : Etudiez la classification retenue. Comparez avec la classification obtenue précédemment avec les Kmodes.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP2-DBSCAN-Correction.html",
    "href": "TP2-DBSCAN-Correction.html",
    "title": "TP 2 - DBSCAN",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées pour la méthode DBSCAN. Les librairies R nécessaires pour ce TP :\nlibrary(mclust)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(dbscan)\nlibrary(seriation)"
  },
  {
    "objectID": "TP2-DBSCAN-Correction.html#reprise-des-données",
    "href": "TP2-DBSCAN-Correction.html#reprise-des-données",
    "title": "TP 2 - DBSCAN",
    "section": "1.1 Reprise des données",
    "text": "1.1 Reprise des données\nOn reprend dans ce second TP les données wine disponibles sur la page moodle du cours. On charge ici les données.\n\nwine&lt;-read.table(\"wine.txt\",header=T)\nwine$Qualite = as.factor(wine$Qualite)\nwine$Type = factor(wine$Type, labels = c(\"blanc\", \"rouge\"))\n\nwineinit&lt;-wine\nwine[,-c(1,2)]&lt;-scale(wine[,-c(1,2)],center=T,scale=T)\n\nhead(wine)\n\n     Qualite  Type      AcidVol    AcidCitr     SO2lbr      SO2tot     Densite\n1352  medium rouge  1.638714588 -1.92626362 -1.2083376 -1.15967786 -0.46497450\n5493  medium blanc -0.068544417 -1.35617574 -0.7004747 -0.85707581 -0.33499781\n5153  medium blanc -0.800226847 -0.59605856  0.5409681 -0.02047014  1.32391517\n5308  medium blanc -0.007570881  0.92417581  1.7824108  1.27893867  1.08790487\n3866  medium blanc  0.419243870  0.03737243 -0.5311870  0.99413674  0.03783006\n694   medium rouge  0.785085086  0.03737243 -0.4747578  0.19313131  1.27260858\n          Alcool\n1352  1.14546909\n5493 -1.12092616\n5153 -1.29526426\n5308 -1.29526426\n3866  0.09944051\n694  -0.94658806\n\n\nOn fait une ACP pour la visualisation des résultats dans la suite\n\nresacp&lt;-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)\nfviz_pca_ind(resacp,habillage=2,geom=c(\"point\"))"
  },
  {
    "objectID": "TP2-DBSCAN-Correction.html#dbscan-à-paramètres-fixés",
    "href": "TP2-DBSCAN-Correction.html#dbscan-à-paramètres-fixés",
    "title": "TP 2 - DBSCAN",
    "section": "1.2 DBSCAN à paramètres fixés",
    "text": "1.2 DBSCAN à paramètres fixés\nQuestion : Dans un premier temps, utilisez l’algorithme DBSCAN avec les paramètres minPts= 7 et eps= 1 à l’aide de la fonction dbscan() de la librairie dbscan. Quels sont les effectifs par classe ? Combien d’individus ne sont pas classés ?\n\n# A COMPLETER\nminPts&lt;-7\neps&lt;-1\nres.db &lt;- dbscan::dbscan(...)\ntable(...)\n\n\nfviz_cluster(res.db, wine[,-c(1:2)], geom=\"point\",ellipse=\"FALSE\")+\n  theme(legend.position=\"none\")+\n  xlab(\"\")+ylab(\"\")+ggtitle(\"Avec DBSCAN\")\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nminPts&lt;-7\neps&lt;-1\nres.db &lt;- dbscan::dbscan(wine[,-c(1,2)],eps=eps,minPts=minPts)\ntable(res.db$cluster)\n\n\n  0   1   2   3   4 \n138 360  75  22   5 \n\nfviz_cluster(res.db, wine[,-c(1:2)], geom=\"point\",ellipse=\"FALSE\")+\n  theme(legend.position=\"none\")+\n  xlab(\"\")+ylab(\"\")+ggtitle(\"Avec DBSCAN\")"
  },
  {
    "objectID": "TP2-DBSCAN-Correction.html#influence-des-paramètres-de-dbscan",
    "href": "TP2-DBSCAN-Correction.html#influence-des-paramètres-de-dbscan",
    "title": "TP 2 - DBSCAN",
    "section": "1.3 Influence des paramètres de DBSCAN",
    "text": "1.3 Influence des paramètres de DBSCAN\nQuestion : Pour étudier l’influence des paramètres minPts et eps, évaluez le nombre de classes obtenues et le nombre d’individus non classés pour différentes valeurs de ces paramètres.\n\nminPts &lt;- ...\neps &lt;- ...\nNBCluster &lt;- matrix(0,nrow=length(minPts),ncol=length(eps))\nNBNonCl &lt;-matrix(0,nrow=length(minPts),ncol=length(eps))\nfor (i in 1:length(minPts)){\n  for (j in 1:length(eps)){\n    res&lt;-dbscan::dbscan(wine[,-c(1,2)], eps=eps[j], minPts=minPts[i])\n    NBCluster[i,j] &lt;- ...\n    NBNonCl[i,j] &lt;- ...\n  }\n}\n\ndf&lt;-data.frame(eps=rep(eps,each=length(minPts)),\n              minPts=as.factor(rep(minPts,length(eps))),\n              NBCluster=c(NBCluster),\n              NBNonCl=c(NBNonCl)*100/nrow(wine))\n\nggplot(df,aes(x=eps,y=NBCluster,col=minPts))+geom_point()+geom_line()\nggplot(df,aes(x=eps,y=NBNonCl,col=minPts))+geom_point()+geom_line()\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nminPts &lt;-seq(5,15,1)\neps &lt;-seq(0.5,2,0.1)\nNBCluster &lt;- matrix(0,nrow=length(minPts),ncol=length(eps))\nNBNonCl &lt;-matrix(0,nrow=length(minPts),ncol=length(eps))\nfor (i in 1:length(minPts)){\n  for (j in 1:length(eps)){\n    res&lt;-dbscan::dbscan(wine[,-c(1,2)], eps=eps[j], minPts=minPts[i])\n    NBCluster[i,j] &lt;- length(table(res$cluster))-1\n    NBNonCl[i,j] &lt;- sum(res$cluster==0)\n  }\n}\n\ndf&lt;-data.frame(eps=rep(eps,each=length(minPts)),\n               minPts=as.factor(rep(minPts,length(eps))),\n               NBCluster=c(NBCluster),\n               NBNonCl=c(NBNonCl)*100/nrow(wine))\n\nggplot(df,aes(x=eps,y=NBCluster,col=minPts))+\n  geom_point()+\n  geom_line()\n\n\n\nggplot(df,aes(x=eps,y=NBNonCl,col=minPts))+\n  geom_point()+\n  geom_line()\n\n\n\n\n\n\nQuestion : Pour une valeur de minPts=7, tracez le graphe de distance kNN afin de choisir le paramètre eps. Vous pouvez utiliser la fonction kNNdistplot(). Qu’en pensez-vous ?\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ndbscan::kNNdistplot(wine[,-c(1,2)], k = 6)\nabline(h=1.5)\n\n\n\nres.db &lt;- dbscan::dbscan(wine[,-c(1,2)],eps=1.5,minPts=7)\ntable(res.db$cluster)\n\n\n  0   1 \n 27 573"
  },
  {
    "objectID": "TP2-DBSCAN-Correction.html#comparaison-avec-les-kmeans",
    "href": "TP2-DBSCAN-Correction.html#comparaison-avec-les-kmeans",
    "title": "TP 2 - DBSCAN",
    "section": "1.4 Comparaison avec les Kmeans",
    "text": "1.4 Comparaison avec les Kmeans\nQuestion : A l’aide des questions précédentes, choisissez des paramètres pour obtenir un clustering à 4 classes. Comparez cette classification avec celle obtenue par les Kmeans pour le même nombre de classes.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nresdb4 &lt;- dbscan::dbscan(wine[,-c(1,2)],eps=0.9,minPts=14)\ntable(resdb4$cluster)\n\n\n  0   1   2   3   4 \n399 149  15  15  22 \n\nfviz_pca_ind(resacp,habillage=as.factor(resdb4$cluster),geom=c(\"point\"))\n\n\n\n\n\nreskmeans4 &lt;- kmeans(wine[,-c(1,2)],4)\ntable(reskmeans4$cluster)\n\n\n  1   2   3   4 \n114 100 218 168 \n\nfviz_pca_ind(resacp,habillage=as.factor(reskmeans4$cluster),geom=c(\"point\"))\n\n\n\n\n\ntable(reskmeans4$cluster,resdb4$cluster)\n\n   \n      0   1   2   3   4\n  1  99   0   0  15   0\n  2  99   1   0   0   0\n  3  82 136   0   0   0\n  4 119  12  15   0  22"
  },
  {
    "objectID": "TP4-MélangeGMM-Correction.html",
    "href": "TP4-MélangeGMM-Correction.html",
    "title": "TP 4 - Classification par modèles de mélanges",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées autour des modèles de mélanges.\nLes librairies R nécessaires pour ce TP :\n## Pour faire le TP\nlibrary(mclust)\nlibrary(Rmixmod)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(reshape2)\n\nlibrary(circlize)\nlibrary(viridis)"
  },
  {
    "objectID": "TP4-MélangeGMM-Correction.html#application-sur-données-simulées-uni-dimensionnelles",
    "href": "TP4-MélangeGMM-Correction.html#application-sur-données-simulées-uni-dimensionnelles",
    "title": "TP 4 - Classification par modèles de mélanges",
    "section": "1.1 Application sur données simulées uni-dimensionnelles",
    "text": "1.1 Application sur données simulées uni-dimensionnelles\nQuestion : A l’aide du code suivant, simulez un jeu de données selon un mélange gaussien en \\(3\\) composantes unidimensionnel. Faites varier les différents paramètres (proportions, moyennes et variances).\n\nset.seed(1234)\na&lt;- ...\nb&lt;- ...\nmu&lt;-c(-a,0,a) # les moyennes \\mu_k\nsigma&lt;-c(b,0.5,b) # les \\sigma_k\nprop&lt;-c(0.2,0.3,0.5)\nn&lt;- ...\n\nZ&lt;-rmultinom(....)\n\nX&lt;-data.frame(X=c(rnorm(...),\n                  rnorm(...),\n                  rnorm(...)))\nlabeltrue&lt;-    # vecteur des vrais labels\n\n\naux&lt;-seq(-(a+4),a+4,0.01)\nY&lt;-data.frame(x=aux,\n              y1=(prop[1]*dnorm(aux,mu[1],sigma[1])), \n              y2=(prop[2]*dnorm(aux,mu[2],sigma[2])),     \n              y3=(prop[3]*dnorm(aux,mu[3],sigma[3])))\n\ngvrai&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=Y,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=Y,col=\"blue\")+\n  geom_line(aes(x=x,y=y3),data=Y,col=\"green\")+\n  theme_minimal()\ngvrai\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nset.seed(1234)\na&lt;-4\nb&lt;-1\nmu&lt;-c(-a,0,a) # les moyennes \\mu_k\nsigma&lt;-c(b,0.5,b) # les \\sigma_k\nprop&lt;-c(0.2,0.3,0.5)\nn&lt;-1000\n\nZ&lt;-rmultinom(n=n,size=1,prob=prop)\n\nX&lt;-data.frame(X=c(rnorm(n=sum(Z[1,]==1),mean=mu[1],sd=sigma[1]),\n                  rnorm(n=sum(Z[2,]==1),mean=mu[2],sd=sigma[2]),\n                  rnorm(n=sum(Z[3,]==1),mean=mu[3],sd=sigma[3])))\nlabeltrue&lt;-c(rep(1,sum(Z[1,]==1)), rep(2,sum(Z[2,]==1)), rep(3,sum(Z[3,]==1)))\n\n\naux&lt;-seq(-(a+4),a+4,0.01)\nY&lt;-data.frame(x=aux,\n              y1=(prop[1]*dnorm(aux,mu[1],sigma[1])), \n              y2=(prop[2]*dnorm(aux,mu[2],sigma[2])),     \n              y3=(prop[3]*dnorm(aux,mu[3],sigma[3])))\n\ngvrai&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=Y,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=Y,col=\"blue\")+\n  geom_line(aes(x=x,y=y3),data=Y,col=\"green\")+\n  theme_minimal()\ngvrai\n\n\n\n\n\n\nQuestion : Estimez les paramètres d’un mélange à \\(K=3\\) classes à l’aide de la fonction Mclust() de la librairie mclust. Comparez la classification obtenue avec les vrais labels.\n\n# A completer\nres&lt;-Mclust(...)\ntable(...,...)\nadjustedRandIndex(...,...)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nres&lt;-Mclust(X$X,G=3)\ntable(res$classification,labeltrue)\n\n   labeltrue\n      1   2   3\n  1 190   2   0\n  2   0 306   1\n  3   0   0 501\n\nadjustedRandIndex(res$classification,labeltrue)\n\n[1] 0.9923714\n\n\n\n\nQuestion : Représentez la densité de mélange estimée sur l’histogramme de l’échantillon simulé. Vous pouvez vous aider de la fonction dnorm() appliquée avec les différentes estimations de paramètres par composante.\n\n# A completer\n# dans y_k &lt;- \\pi_k \\times \\phi(x; \\mu_k,\\sigma_k^2)\n\nMelEstim&lt;-data.frame(x=aux,\n                     y1=....., \n                     y2=.....,\n                     y3=......)\nMelEstim&lt;-data.frame(MelEstim,Somme=apply(MelEstim[,2:4],1,sum))\n\ngMelEst&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=MelEstim,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=MelEstim,col=\"green4\")+\n  geom_line(aes(x=x,y=y3),data=MelEstim,col=\"blue\")+\n  geom_line(aes(x=x,y=Somme),data=MelEstim,col=\"yellow\",linetype = \"dashed\",linewidth=1.5)\ngMelEst\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nMelEstim&lt;-data.frame(x=aux,\n                     y1=res$parameters$pro[1] * dnorm(aux,res$parameters$mean[1],sqrt(res$parameters$variance$sigmasq[1])),\n                     y2=res$parameters$pro[2] * dnorm(aux,res$parameters$mean[2],sqrt(res$parameters$variance$sigmasq[2])),\n                     y3=res$parameters$pro[3] * dnorm(aux,res$parameters$mean[3],sqrt(res$parameters$variance$sigmasq[3])))\nMelEstim&lt;-data.frame(MelEstim,Somme=apply(MelEstim[,2:4],1,sum))\n\ngMelEst&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=MelEstim,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=MelEstim,col=\"green4\")+\n  geom_line(aes(x=x,y=y3),data=MelEstim,col=\"blue\")+\n  geom_line(aes(x=x,y=Somme),data=MelEstim,col=\"yellow\",linetype = \"dashed\",linewidth=1.5)\ngMelEst\n\n\n\n\n\n\nQuestion : Calculez les probabilités a posteriori d’appartenance des individus à chacune des trois classes et tracez-les graphiquement.\n\n# dans p mettre le vecteur des proba a posteriori d'appartenance (t_{11},\\ldots,t_{n1},t_{12},\\ldots,t_{n3})\n\nMelProba&lt;-data.frame(x=rep(aux,3),\n                     p= c(..., ..., ..),    \n                     class=as.factor(rep(c(1,2,3),each=length(aux))))\n\ngprobapost&lt;-ggplot(MelProba,aes(x=x,y=p,col=class))+geom_line()\n\ngprobapost\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nMelProba&lt;-data.frame(x=rep(aux,3),\n                     p=c(MelEstim$y1 / MelEstim$Somme , \n                         MelEstim$y2 / MelEstim$Somme,\n                         MelEstim$y3 / MelEstim$Somme),\n                     class=as.factor(rep(c(1,2,3),each=length(aux))))\n\ngprobapost&lt;-ggplot(MelProba,aes(x=x,y=p,col=class))+geom_line()\n\ngprobapost\n\n\n\n\n\n\nQuestion : Tracez les boxplots des probabilités d’appartenance maximales par classe. Vous pouvez vous aider de la fonction apply().\n\ndf&lt;-data.frame(lab=...,probamax=...)\ngprobamax&lt;-ggplot(df,aes(x=lab,y=probamax))+geom_boxplot()\ngrid.arrange(gvrai,gMelEst,gprobapost,gprobamax,ncol=2)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ngprobamax&lt;-ggplot(data.frame(lab=as.factor(apply(res$z,1,which.max)),\n                          probamax=apply(res$z,1,max)),\n                    aes(x=lab,y=probamax))+geom_boxplot()\n\ngrid.arrange(gvrai,gMelEst,gprobapost,gprobamax,ncol=2)"
  },
  {
    "objectID": "TP4-MélangeGMM-Correction.html#application-sur-des-données-simulées-dans-mathbbr2",
    "href": "TP4-MélangeGMM-Correction.html#application-sur-des-données-simulées-dans-mathbbr2",
    "title": "TP 4 - Classification par modèles de mélanges",
    "section": "1.2 Application sur des données simulées dans \\(\\mathbb{R}^2\\)",
    "text": "1.2 Application sur des données simulées dans \\(\\mathbb{R}^2\\)\nOn va ici utiliser les données simulées “ex4.1” disponibles dans la librairie mclust. Ces données sont simulées selon un mélange de densités gaussiennes, proposées dans Baudry et al (2010). L’objectif est d’étudier l’impact du choix des formes des mélanges considérées et de la différence d’objectif entre les critères BIC et ICL. On va au travers de ce jeu de données simulées simple appréhender la manipulation des fonctions pour le clustering par mélanges gaussiens avec R.\nOn commence ici par charger les données :\n\nlibrary(mclust)\ndata(Baudry_etal_2010_JCGS_examples)\nData&lt;-ex4.1\nggplot(Data,aes(x=X1,y=X2))+geom_point()\n\n\n\n\n\n1.2.1 Mélanges gaussiens diagonaux\nDans cette section, on va considérer une collection de modèles de mélanges gaussiens avec un nombre de composantes \\(K\\) variant entre \\(2\\) et \\(10\\) et des matrices de variance-covariance diagonales.\nQuestion : A l’aide de la fonction Mclust(), estimez les paramètres des mélanges gaussiens considérés. Vous pouvez consulter l’aide de la fonction mclustModelNames() pour le choix des formes des mélanges.\n\n# A COMPLETER\nresBICdiag&lt;-Mclust(....)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nresBICdiag&lt;-Mclust(Data,G=2:10,modelNames=c(\"EII\",\"VII\",\"EEI\",\"VEI\",\"EVI\",\"VVI\"))\n\n\n\nQuestion : A l’aide de la fonction fviz_mclust_bic(), visualisez le comportement du critère BIC sur la collection de modèles. Quel modèle est sélectionné ? Contrôlez à l’aide de summary(resBICdiag).\n\nfviz_mclust(....,what=....)\nsummary(resBICdiag)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nfviz_mclust(resBICdiag,what=\"BIC\")\n\n\n\nsummary(resBICdiag)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust EVI (diagonal, equal volume, varying shape) model with 10 components: \n\n log-likelihood   n df       BIC       ICL\n      -1993.944 600 40 -4243.764 -4470.305\n\nClustering table:\n  1   2   3   4   5   6   7   8   9  10 \n 48  70  82 145  16  53  74   8  72  32 \n\n\n\n\nQuestion : Tracez la classification obtenue sur le nuage de points (vous pouvez utiliser la fonction fviz_cluster()). Comment est obtenue cette classification à partir du mélange gaussien retenu ? Quels sont les effectifs par classe ? Contrôlez les probabilités a posterori d’appartenance.\n\n# Visualisation du clustering\nfviz_cluster(...)\n# Effectifs par classe\ntable(....)\n# Boxplot des probabilités a posteriori maximales\nAux&lt;-data.frame(label=...,    proba=...)\nggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\n\n\n\n\n\n\n\nCorrection\n\n\n\n\n# Effectifs par classe\ntable(resBICdiag$classification)\n\n\n  1   2   3   4   5   6   7   8   9  10 \n 48  70  82 145  16  53  74   8  72  32 \n\nAux&lt;-data.frame(label=paste(\"Cl\",resBICdiag$classification,sep=\"\"),    proba=apply(resBICdiag$z,1,max))\n# Boxplot des probabilités a posteriori maximales\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\n# Classification sur les points\nh2&lt;-fviz_cluster(resBICdiag,data=Data,ellipse.type=\"norm\",geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\n\n\n\nQuestion : Quel mélange gaussien est retenu avec le critère ICL ? Vous utiliserez la fonction mclustICL(). Etudiez la classification alors déduite.\n\nresICL&lt;-mclustICL(...)\nsummary(resICL)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nresICL&lt;-mclustICL(Data,G=2:10,modelNames=c(\"EII\",\"VII\",\"EEI\",\"VEI\",\"EVI\",\"VVI\"))\nsummary(resICL)\n\nBest ICL values:\n             EVI,4       VVI,4      EEI,4\nICL      -4407.029 -4422.87307 -4446.3616\nICL diff     0.000   -15.84396   -39.3325\n\n\n\nmodICL&lt;-Mclust(Data,G=4,modelNames = \"EVI\")\nAux&lt;-data.frame(label=paste(\"Cl\",modICL$classification,sep=\"\"), proba=apply(modICL$z,1,max))\nggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\n\n\n\nplot(modICL,what=\"classification\")\n\n\n\n\n\n\n\n\n1.2.2 Toutes les formes de mélanges gaussiens\nQuestion : Reprenez les questions de la section précédente en considérant ici toutes les formes de mélanges gaussiens. Commentez.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\nChoix avec le critère BIC :\n\nresBICall&lt;-Mclust(Data,G=2:10)\n#plot(resBICall)\nfviz_mclust(resBICall,what=\"BIC\")\n\n\n\nsummary(resBICall)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust EEV (ellipsoidal, equal volume and shape) model with 6 components: \n\n log-likelihood   n df       BIC       ICL\n      -1957.781 600 25 -4075.484 -4248.353\n\nClustering table:\n  1   2   3   4   5   6 \n 78 122 121 107 132  40 \n\nAux&lt;-data.frame(label=paste(\"Cl\",resBICall$classification,sep=\"\"), proba=apply(resBICall$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resBICall,data=Data,ellipse.type=\"norm\",geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\n\nChoix avec le critère ICL :\n\nresICLall&lt;-mclustICL(Data,G=2:10)\nsummary(resICLall)\n\nBest ICL values:\n             VVV,4       EVV,4       EEV,6\nICL      -4230.492 -4240.60219 -4248.35295\nICL diff     0.000   -10.10982   -17.86057\n\nresICL&lt;-Mclust(Data,G=4,modelNames = \"VVV\")\n\nAux&lt;-data.frame(label=paste(\"Cl\",resICL$classification,sep=\"\"), proba=apply(resICL$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resICL,data=Data,ellipse.type=\"norm\",geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)"
  },
  {
    "objectID": "TP4-MélangeGMM-Correction.html#etude-des-données-de-vins",
    "href": "TP4-MélangeGMM-Correction.html#etude-des-données-de-vins",
    "title": "TP 4 - Classification par modèles de mélanges",
    "section": "1.3 Etude des données de vins",
    "text": "1.3 Etude des données de vins\nOn reprend dans ce TP les données wine disponibles sur la page moodle du cours. On charge ici les données.\n\nwine&lt;-read.table(\"wine.txt\",header=T)\nwine$Qualite = as.factor(wine$Qualite)\nwine$Type = factor(wine$Type, labels = c(\"blanc\", \"rouge\"))\n\nwineinit&lt;-wine\nwine[,-c(1,2)]&lt;-scale(wine[,-c(1,2)],center=T,scale=T)\n\nhead(wine)\n\n     Qualite  Type      AcidVol    AcidCitr     SO2lbr      SO2tot     Densite\n1352  medium rouge  1.638714588 -1.92626362 -1.2083376 -1.15967786 -0.46497450\n5493  medium blanc -0.068544417 -1.35617574 -0.7004747 -0.85707581 -0.33499781\n5153  medium blanc -0.800226847 -0.59605856  0.5409681 -0.02047014  1.32391517\n5308  medium blanc -0.007570881  0.92417581  1.7824108  1.27893867  1.08790487\n3866  medium blanc  0.419243870  0.03737243 -0.5311870  0.99413674  0.03783006\n694   medium rouge  0.785085086  0.03737243 -0.4747578  0.19313131  1.27260858\n          Alcool\n1352  1.14546909\n5493 -1.12092616\n5153 -1.29526426\n5308 -1.29526426\n3866  0.09944051\n694  -0.94658806\n\n\nOn fait une ACP pour la visualisation des résultats dans la suite\n\nresacp&lt;-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)\n\nQuestion : Déterminez une classification de ces données à l’aide d’un modèle de mélange. Comparez votre résultat avec les classifications obtenues dans les TP précédents (avec Kmeans, CAH).\n\n# A FAIRE\n\n\n\n\n\n\n\nCorrection\n\n\n\nAvec le critère BIC :\n\nset.seed(1234)\nresBICall&lt;-Mclust(wine[,-c(1,2)],G=2:20)\nfviz_mclust(resBICall,what=\"BIC\")\n\n\n\nsummary(resBICall)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVE (ellipsoidal, equal orientation) model with 9 components: \n\n log-likelihood   n  df       BIC       ICL\n      -3817.907 600 131 -8473.812 -8644.462\n\nClustering table:\n  1   2   3   4   5   6   7   8   9 \n 94  51  32 106  22 156  63  47  29 \n\nresBIC&lt;-Mclust(wine[,-c(1,2)],G=9,modelNames = \"VVE\")\nAux&lt;-data.frame(label=paste(\"Cl\",resBICall$classification,sep=\"\"), proba=apply(resBICall$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resBICall,data=Data,ellipse.type=\"norm\",geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\n\nAvec le critère ICL :\n\nresICLall&lt;-mclustICL(wine[,-c(1,2)],G=2:20)\nsummary(resICLall)\n\nBest ICL values:\n             EVV,3       VVE,9       EEV,3\nICL      -8628.533 -8644.46196 -8648.00671\nICL diff     0.000   -15.92897   -19.47373\n\nresICL&lt;-Mclust(wine[,-c(1,2)],G=3,modelNames = \"EVV\")\n\nAux&lt;-data.frame(label=paste(\"Cl\",resICL$classification,sep=\"\"), proba=apply(resICL$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resICL,data=Data,ellipse.type=\"norm\",geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\ndf&lt;-data.frame(wine[,-c(1,2)],Class=as.factor(resICL$classification))\ndf&lt;-melt(df,id=\"Class\")\nggplot(df,aes(x=variable,y=value))+geom_violin(aes(fill=Class))\n\n\n\n\n\n# comparaison des deux classifications\ntable(resICL$classification,resBIC$classification)\n\n   \n      1   2   3   4   5   6   7   8   9\n  1  92   0   0  26   0   0   0   0  29\n  2   1  30  27  60   2  10   0   0   0\n  3   1  21   5  20  20 146  63  47   0\n\nadjustedRandIndex(resICL$classification,resBIC$classification)\n\n[1] 0.2828314"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html",
    "href": "TP1-Kmeans-Correction.html",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées dans le chapitre dédié aux algorithmes de clustering de type Kmeans. Les librairies R nécessaires pour ce TP :\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(mclust)\nlibrary(cluster)\nlibrary(ppclust)\nlibrary(ggalluvial)\nlibrary(klaR)\nlibrary(gridExtra)\n#library(circlize)\n#library(viridis)\nlibrary(reshape2)"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#analyse-descriptive-des-données",
    "href": "TP1-Kmeans-Correction.html#analyse-descriptive-des-données",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "1.1 Analyse descriptive des données",
    "text": "1.1 Analyse descriptive des données\n\n1.1.1 Présentation des données de vins\nDans ce TP, on va utiliser le jeu de données wine disponible sur la page moodle du cours.\nCe jeu de données comprend des mesures physico-chimiques réalisées sur un échantillon de \\(n=600\\) vins (rouges et blancs) du Portugal. Ces mesures sont complétées par une évaluation sensorielle de la qualité par un ensemble d’experts. Chaque vin est décrit par les variables suivantes :\n\nQualite : son évaluation sensorielle par les experts (“bad”,“medium”,“good”),\nType : son type (1 pour un vin rouge, 0 pour un vin blanc),\nAcidVol : la teneur en acide volatile (en g/dm3 d’acide acétique),\nAcidCitr : la teneur en acide citrique (en g/dm3),\nSO2lbr : le dosage du dioxyde de soufre libre (en mg/dm3),\nSO2tot : le dosage du dioxyde de soufre total (en mg/dm3),\nDensite : la densité (en g/cm3),\nAlcool : le degré d’alcool (en % Vol.).\n\nQuestion 1. Récupérez sur moodle le jeu de données wine.txt et chargez-le sous R.\n\nwine &lt;-read.table(.......)\n\nVérifiez la nature des variables à l’aide de la fonction str(). Modifiez si nécessaire les variables qualitatives (à l’aide de as.factor()) et transformez les modalités “1” et “0” de la variable Typeen “rouge” et “blanc” respectivement (à l’aide de la fonction factor()).\n\nwine$Qualite &lt;- ....\nwine$Type &lt;- factor(....)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nwine&lt;-read.table(\"wine.txt\",header=T)\nstr(wine)\n\n'data.frame':   600 obs. of  8 variables:\n $ Qualite : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ Type    : int  1 0 0 0 0 1 0 0 0 0 ...\n $ AcidVol : num  0.62 0.34 0.22 0.35 0.42 0.48 0.21 0.28 0.3 0.4 ...\n $ AcidCitr: num  0.01 0.1 0.22 0.46 0.32 0.32 0.32 0.14 0.25 0.42 ...\n $ SO2lbr  : num  8 17 39 61 20 21 39 64 21 41 ...\n $ SO2tot  : int  46 63 110 183 167 122 113 159 124 176 ...\n $ Densite : num  0.993 0.994 0.999 0.998 0.995 ...\n $ Alcool  : num  11.8 9.2 9 9 10.6 9.4 10.2 10 10.8 9.4 ...\n\nwine$Qualite &lt;- as.factor(wine$Qualite)\nwine$Type &lt;- factor(wine$Type, labels = c(\"blanc\", \"rouge\"))\ndim(wine)\n\n[1] 600   8\n\nhead(wine)\n\n     Qualite  Type AcidVol AcidCitr SO2lbr SO2tot Densite Alcool\n1352  medium rouge    0.62     0.01      8     46 0.99332   11.8\n5493  medium blanc    0.34     0.10     17     63 0.99370    9.2\n5153  medium blanc    0.22     0.22     39    110 0.99855    9.0\n5308  medium blanc    0.35     0.46     61    183 0.99786    9.0\n3866  medium blanc    0.42     0.32     20    167 0.99479   10.6\n694   medium rouge    0.48     0.32     21    122 0.99840    9.4\n\n\n\n\n\n\n1.1.2 Statistiques descriptives\nQuestion 2. Faites quelques statistiques descriptives pour faire connaissance avec le jeu de données, avec des choix adaptés à la nature des variables. En particulier, étudiez les corrélations entre les variables quantitatives et faites une ACP.\n\n# A completer\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nsummary(wine)\n\n   Qualite       Type        AcidVol          AcidCitr          SO2lbr      \n bad   : 19   blanc:425   Min.   :0.1000   Min.   :0.0000   Min.   :  2.00  \n good  :110   rouge:175   1st Qu.:0.2400   1st Qu.:0.2400   1st Qu.: 15.75  \n medium:471               Median :0.3000   Median :0.3000   Median : 27.00  \n                          Mean   :0.3512   Mean   :0.3141   Mean   : 29.41  \n                          3rd Qu.:0.4300   3rd Qu.:0.3900   3rd Qu.: 41.00  \n                          Max.   :1.0400   Max.   :1.0000   Max.   :112.00  \n     SO2tot         Densite           Alcool     \n Min.   :  7.0   Min.   :0.9875   Min.   : 8.00  \n 1st Qu.: 68.0   1st Qu.:0.9925   1st Qu.: 9.50  \n Median :114.5   Median :0.9949   Median :10.40  \n Mean   :111.2   Mean   :0.9947   Mean   :10.49  \n 3rd Qu.:154.0   3rd Qu.:0.9970   3rd Qu.:11.30  \n Max.   :278.0   Max.   :1.0030   Max.   :14.00  \n\n\nOn trace les barplots pour les variables qualitatives\n\nggplot(wine) + \n  geom_bar(aes(x = Type, y = ..prop.., group = 1))+\n  ggtitle(\"Frequences\")\n\n\n\nwine$Qualite_rec &lt;- fct_relevel(wine$Qualite, \"bad\", \"medium\", \"good\")\nggplot(wine) + \n  geom_bar(aes(x = Qualite_rec, y = ..prop.., group = 1))+\n  ggtitle(\"Frequences\")+xlab(\"Qualite\")\n\n\n\nwine&lt;-wine[,-9]\n\nOn trace les violin-plots pour chacune des variables quantitatives :\n\nggplot(melt(wine[,c(3,4,8)]),aes(x=variable,y=value))+\n  geom_violin()\n\n\n\nggplot(melt(wine[,c(5,6)]),aes(x=variable,y=value))+\n  geom_violin()\n\n\n\nggplot(data.frame(variable=rep(\"Densite\",nrow(wine)),value=wine$Densite),aes(x=variable,y=value))+\n  geom_violin()\n\n\n\n\nOn peut regarder également les corrélations entre variables quantitatives :\n\ncorrplot(cor(wine[,-c(1,2)]),method=\"ellipse\")\n\n\n\n\nACP pour visualiser les données :\n\nresacp&lt;-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)\n# Visualisation des valeurs propres\nfviz_eig(resacp)\n\n\n\n# Visualisation des variables\nfviz_pca_var(resacp,axes=c(1,2))\n\n\n\n# Projection des individus\nfviz_pca_ind(resacp,axes=c(1,2),\n             geom = c(\"point\"),\n             habillage=wine$Type,\n             repel=T)\n\n\n\nfviz_pca_ind(resacp,axes=c(1,3),\n             geom = c(\"point\"),\n             habillage=wine$Type,\n             repel=T)\n\n\n\n\n\n\nQuestion : Pour la suite, on va utiliser les variables quantitatives pour faire de la classification non supervisée des vins. Les variables Qualite et Type seront utilisées comme des variables extérieures pour comparer / croiser avec les classifications obtenues pour l’interprétation.\nPensez-vous qu’il est nécessaire de transformer les variables quantitatives dans l’objectif de clustering avec un algorithme des Kmeans ? Si oui, mettez en place cette transformation.\n\n# A completer\n\n\n\n\n\n\n\nCorrection\n\n\n\nEn raison des différences d’échelle entre les différentes variables, on va centrer-réduire les données.\n\nwineinit&lt;-wine\nwine[,-c(1,2)]&lt;-scale(wine[,-c(1,2)],center=T,scale=T)"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-des-kmeans",
    "href": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-des-kmeans",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "1.2 Classification avec l’algorithme des Kmeans",
    "text": "1.2 Classification avec l’algorithme des Kmeans\n\n1.2.1 A K=3 fixé\nQuestion : A l’aide de la fonction kmeans(), faites une classification non supervisée en 3 classes des vins. Regardez les options disponibles dans la fonction kmeans().\n\nhelp(kmeans)\nreskmeans&lt;-kmeans(....)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nK&lt;-3\nreskmeans&lt;-kmeans(wine[,-c(1,2)],centers=K)  \n\n\n\nQuestion : Combien a-ton de vins par classe ? Visualisez la classification obtenue dans les premiers plans de l’ACP (vous pouvez utiliser la fonction PCA() de la librairie FactoMineR et la fonction fviz_cluster de la librairie factoextra).\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ntable(reskmeans$cluster)\n\n\n  1   2   3 \n170 182 248 \n\nfviz_cluster(reskmeans,data=wine[,-c(1,2)],ellipse.type=\"norm\",labelsize=8,geom=c(\"point\"))+ggtitle(\"\")\n\n\n\nfviz_pca_ind(resacp,col.ind=as.factor(reskmeans$cluster),\n             geom = c(\"point\"),axes=c(1,2))\n\n\n\nfviz_pca_ind(resacp,col.ind=as.factor(reskmeans$cluster),\n             geom = c(\"point\"),axes=c(1,3))\n\n\n\nfviz_pca_ind(resacp,col.ind=as.factor(reskmeans$cluster),\n             geom = c(\"point\"),axes=c(2,3))\n\n\n\n\n\n\nQuestion : La classification obtenue précédemment a-t-elle un lien avec le type de vins ? Avec la qualité du vin ? Vous pouvez vous aider de la fonction table(), la fonction adjustedRandIndex() de la librairie mclust, …\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ntable(reskmeans$cluster,wine$Type)\n\n   \n    blanc rouge\n  1    21   149\n  2   173     9\n  3   231    17\n\nadjustedRandIndex(reskmeans$cluster,wine$Type)\n\n[1] 0.3552072\n\ntable(reskmeans$cluster,wine$Qualite)\n\n   \n    bad good medium\n  1   7   15    148\n  2   4   21    157\n  3   8   74    166\n\nadjustedRandIndex(reskmeans$cluster,wine$Qualite)\n\n[1] -0.001572137\n\n\n\n\n\n\n1.2.2 Choix du nombre de classes\nQuestion : On s’intéresse dans cette section au choix du nombre de classes \\(K\\) en étudiant l’évolution de l’inertie intraclasse. En faisant varier \\(K\\) entre 2 et 15, calculez l’inertie intraclasse associée à chaque classification obtenue. Tracez l’évolution de l’inertie intraclasse en fonction du nombre de classes. Qu’en concluez-vous ?\n\n# A completer\nKmax&lt;-15\nreskmeanscl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nIintra&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-kmeans(...)\n  reskmeanscl[,k-1]&lt;-resaux$...\n  Iintra&lt;-c(Iintra,resaux$...)\n}\n\ndf&lt;-data.frame(K=2:15,Iintra=Iintra)\nggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()+xlab(\"Nombre de classes\")+ylab(\"Inertie intraclasse\")\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nKmax&lt;-15\nreskmeanscl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nIintra&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-kmeans(wine[,-c(1:2)],centers=k)\n  reskmeanscl[,k-1]&lt;-resaux$cluster\n  Iintra&lt;-c(Iintra,resaux$tot.withinss)\n}\n\ndf&lt;-data.frame(K=2:15,Iintra=Iintra)\nggplot(df,aes(x=K,y=Iintra))+\n  geom_line()+\n  geom_point()+\n  xlab(\"Nombre de classes\")+\n  ylab(\"Inertie intraclasse\")\n\n\n\n\n\n\nQuestion : Reprendre la question du choix du nombre de classes en utilisant le critère silhouette (vous pouvez vous aider de la fonction silhouette() de la librairie cluster). Pour la classification sélectionnée, représentez les poids \\(s(i)\\) de chaque individu à l’aide de la fonction fviz_silhouette().\n\n# A COMPLETER\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n   aux&lt;-silhouette(..., daisy(wine[,-c(1,2)]))\n   Silhou&lt;-c(Silhou,mean(aux[,3]))\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\naux&lt;-silhouette(...)\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\nrm(df,Silhou,aux)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n   aux&lt;-silhouette(reskmeanscl[,k-1], daisy(wine[,-c(1,2)]))\n   Silhou&lt;-c(Silhou,mean(aux[,3]))\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\n\n\naux&lt;-silhouette(reskmeanscl[,3], daisy(wine[,-c(1:2)]))\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\n\n  cluster size ave.sil.width\n1       1  218          0.33\n2       2  168          0.29\n3       3  114          0.29\n4       4  100          0.18\n\n\n\n\n\n\nrm(df,Silhou,aux)"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-pam",
    "href": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-pam",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "1.3 Classification avec l’algorithme PAM",
    "text": "1.3 Classification avec l’algorithme PAM\nQuestion : Déterminez une classification en \\(K=3\\) classes des vins en utilisant la méthode PAM (fonction pam()de la librairie cluster) et représentez graphiquement la classification obtenue. A-t-elle un lien avec le type de vins ? Avec la qualité ? Avec la classification en \\(K=3\\) classes obtenue avec la méthode des Kmeans?\n\n# A COMPLETER\n\nresPAM&lt;-pam(x=....,k=..,metric=...)\nresPAM$medoids\nresPAM$id.med\n\nfviz_cluster(resPAM,data=wine[,-c(1,2)],ellipse.type=\"norm\",labelsize=8,geom=c(\"point\"))+ggtitle(\"\")\nfviz_pca_ind(resacp,col.ind=as.factor(resPAM$clustering),geom = c(\"point\"),axes=c(1,2))\n\nadjustedRandIndex(......)\ntable(.......)\n\nclust1&lt;-paste(\"K-k\",......,sep=\"\")\nclust2&lt;-paste(\"PAM-k\",......,sep=\"\")\nTab&lt;-melt(table(clust1,clust2))\nggplot(Tab,aes(y=value,axis1=clust1,axis2=clust2))+\n  geom_alluvium(aes(fill=clust1))+\n  geom_stratum(width = 1/12)+   \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nresPAM&lt;-pam(wine[,-c(1,2)], 3, metric = \"euclidean\")\nresPAM$medoids\n\n        AcidVol    AcidCitr      SO2lbr     SO2tot    Densite     Alcool\n1394  1.0289792 -0.40602926 -0.58761623 -0.9282763  0.4927485 -0.2492357\n1604 -0.7392533  0.03737243  0.99240181  1.3323390  0.3148856 -0.5107428\n2155 -0.6173062  0.16405862  0.08953436 -0.3230722 -1.0532900  0.7096239\n\nresPAM$id.med\n\n[1] 562 316  22\n\nfviz_cluster(resPAM,data=wine[,-c(1,2)],ellipse.type=\"norm\",labelsize=8,geom=c(\"point\"))+ggtitle(\"\")\n\n\n\nfviz_pca_ind(resacp,col.ind=as.factor(resPAM$clustering),geom = c(\"point\"),axes=c(1,2))\n\n\n\nadjustedRandIndex(resPAM$clustering,wine$Type)\n\n[1] 0.3553001\n\ntable(resPAM$clustering,wine$Type)\n\n   \n    blanc rouge\n  1    34   163\n  2   177     1\n  3   214    11\n\nadjustedRandIndex(resPAM$clustering,wine$Qualite)\n\n[1] 0.01383852\n\ntable(resPAM$clustering,wine$Qualite)\n\n   \n    bad good medium\n  1  11   15    171\n  2   4   23    151\n  3   4   72    149\n\nadjustedRandIndex(resPAM$clustering,reskmeans$cluster)\n\n[1] 0.7990222\n\ntable(resPAM$clustering,reskmeans$cluster)\n\n   \n      1   2   3\n  1 170  11  16\n  2   0 167  11\n  3   0   4 221\n\n\n\nclust1&lt;-paste(\"K-k\",reskmeanscl[,2],sep=\"\")\nclust2&lt;-paste(\"PAM-k\",resPAM$clustering,sep=\"\")\nTab&lt;-melt(table(clust1,clust2))\nggplot(Tab,aes(y=value,axis1=clust1,axis2=clust2))+\n  geom_alluvium(aes(fill=clust1))+\n  geom_stratum(width = 1/12)+   \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  theme(legend.position = \"none\")\n\n\n\n\n\nrm(resPAM)\n\n\n\nQuestion : Déterminez le nombre de classes optimal par le critère Silhouette pour \\(K\\) variant entre 2 et 15 avec l’algorithme PAM. Commentez la classification retenue. Est-elle proche de celle obtenue avec l’algorithme des Kmeans ?\n\n# A completer\n\nKmax&lt;-15\nresPAMcl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-pam(.....)\n  resPAMcl[,k-1]&lt;-resaux$clustering\n  aux&lt;-silhouette(resPAMcl[,k-1], daisy(wine[,-c(1,2)]))\n  Silhou&lt;-c(Silhou,......)\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\n\naux&lt;-silhouette(resPAMcl[,1], daisy(wine[,-c(1:2)]))\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\n\nadjustedRandIndex(.....)\ntable(.....)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nKmax&lt;-15\nresPAMcl&lt;-matrix(0,nrow=nrow(wine),ncol=Kmax-1)\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-pam(wine[,-c(1:2)],k,metric=\"euclidean\")\n  resPAMcl[,k-1]&lt;-resaux$clustering\n  aux&lt;-silhouette(resPAMcl[,k-1], daisy(wine[,-c(1,2)]))\n  Silhou&lt;-c(Silhou,mean(aux[,3]))\n}\n\ndf&lt;-data.frame(K=2:Kmax,Silhouette=Silhou)\nggplot(df,aes(x=K,y=Silhouette))+\n  geom_point()+\n  geom_line()+theme(legend.position = \"bottom\")\n\n\n\naux&lt;-silhouette(resPAMcl[,1], daisy(wine[,-c(1:2)]))\nfviz_silhouette(aux)+theme(plot.title = element_text(size =9))\n\n  cluster size ave.sil.width\n1       1  203          0.23\n2       2  397          0.28\n\n\n\n\nadjustedRandIndex(resPAMcl[,1],reskmeanscl[,3])\n\n[1] 0.358877\n\ntable(resPAMcl[,1],reskmeanscl[,3])\n\n   \n      1   2   3   4\n  1  13   3 114  73\n  2 205 165   0  27\n\ntable(resPAMcl[,1],wine$Type)\n\n   \n    blanc rouge\n  1    37   166\n  2   388     9\n\n\n\nclust1&lt;-paste(\"K-k\",reskmeanscl[,3],sep=\"\")\nclust2&lt;-paste(\"PAM-k\",resPAMcl[,1],sep=\"\")\nTab&lt;-melt(table(clust1,clust2))\nggplot(Tab,aes(y=value,axis1=clust1,axis2=clust2))+\n  geom_alluvium(aes(fill=clust1))+\n  geom_stratum(width = 1/12)+   \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-fuzzy-c-means",
    "href": "TP1-Kmeans-Correction.html#classification-avec-lalgorithme-fuzzy-c-means",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "1.4 Classification avec l’algorithme fuzzy c-means",
    "text": "1.4 Classification avec l’algorithme fuzzy c-means\n\n1.4.1 Présentation\nAvec les algorithmes de clustering précédents (Kmeans, PAM) nous obtenons une classification “dure” au sens que chaque individu ne peut appartenir qu’à une seule classe et chaque individu participe avec le même poids à la construction des classes. Une classification dure \\(\\mathcal{P}_K=\\{\\mathcal{C}_1,\\ldots,\\mathcal{C}_K\\}\\) peut se traduire en une matrice \\(Z=(z_{ik})_{\\underset{1\\leq k \\leq K}{1\\leq i \\leq n}}\\) avec \\(z_{ik}=1\\) si \\(i\\in\\mathcal{C}_k\\) et 0 sinon. Dans cette section, nous allons nous intéresser à une adaptation de l’algorithme des Kmeans, appelée fuzzy c-means. L’idée est de retourner une classification fuzzy c’est-à-dire une matrice \\(W=(\\omega_{ik})_{\\underset{1\\leq k \\leq K}{1\\leq i \\leq n}}\\) avec \\(\\forall i,\\ k,\\ \\omega_{ik}\\geq 0\\) et \\(\\forall i,\\ \\underset{k=1}{\\stackrel{K}{\\sum}} \\omega_{ik}=1\\). On donne ainsi plutôt un poids \\(\\omega_{ik}\\) que l’individu \\(i\\) appartienne à la classe \\(\\mathcal{C}_k\\).\nL’algorithme fuzzy c-means a pour fonction objective\n\\[\n\\underset{W,\\{m_1,\\ldots,m_K\\}}{\\mbox{argmin}}\\ \\underset{i=1}{\\stackrel{n}{\\sum}}\\underset{k=1}{\\stackrel{K}{\\sum}} (\\omega_{ik})^\\gamma\\ \\|x_i - m_k\\|^2\n\\] où \\(X=(x_1,\\ldots,x_n)'\\) est la matrice des données, \\(\\gamma\\in[1,+\\infty[\\), \\(m_k\\) est le centre de la classe \\(\\mathcal{C}_k\\).\nDans le même principe que l’algorithme des Kmeans, l’algorithme fuzzy c-means est un algorithme itératif :\n\nStep 1: Initialisation des poids \\(W^{(0)}\\)\nStep 2: A l’itération \\(r\\), on calcule les centres des classes\n\n\\[\nm_k^{(r)} = \\frac{\\underset{i=1}{\\stackrel{n}{\\sum}} (\\omega_{ik}^{(r-1)})^\\gamma x_i}{\\underset{i=1}{\\stackrel{n}{\\sum}} (\\omega_{ik}^{(r-1)})^\\gamma}\n\\]\n\nStep 3: Mise à jour des poids (\\(\\gamma&gt;1\\)) \\[\n\\omega_{ik}^{(r)} = \\left[\\underset{\\ell=1}{\\stackrel{K}{\\sum}} \\left(\\frac{\\|x_i - m_k^{(r)}\\|^2}{\\|x_i - m_\\ell^{(r)}\\|^2}\\right)^{\\frac{1}{\\gamma-1}}  \\right]^{-1}\n\\]\nStep 4: Si \\(\\|W^{(r)} - W^{(r-1)}\\|&lt;\\) seuil, on s’arrête, sinon on retourne à l’étape 2.\n\nEn général, la puissance choisie sur les poids est \\(\\gamma=2\\). Dans le cas \\(\\gamma=1\\), on retrouve l’algorithme des Kmeans.\n\n\n1.4.2 Application avec R\nNous allons ici nous appuyer sur la fonction fcm() de la librairie ppclust.\nQuestion : Utilisez cet algorithme pour obtenir une classification en \\(3\\) classes. Comment sont initialisés les poids ? Comment est obtenue la classification finale ? A l’aide des poids, étudiez la stabilité des classes. Vous pouvez pour cela étudier les poids des individus par classe.\n\n# A COMPLETER\nlibrary(ppclust)\nresfcm&lt;-fcm(...)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nlibrary(ppclust)\nresfcm&lt;-fcm(wine[,-c(1,2)],centers= 3, m=2)\n\ntable(apply(resfcm$u,1,which.max))\n\n\n  1   2   3 \n230 175 195 \n\ntable(resfcm$cluster)\n\n\n  1   2   3 \n230 175 195 \n\nAux&lt;-data.frame(cluster=as.factor(resfcm$cluster),PoidsMax=apply(resfcm$u,1,max))\nggplot(Aux,aes(x=cluster,y=PoidsMax))+geom_boxplot()\n\n\n\n\n\n\nQuestion : Représentez la classification obtenue sur le premier plan de l’ACP en nuançant selon les poids.\n\n# A COMPLETER\nfviz_pca_ind(resacp,axes=c(1,2),geom=c(\"point\"),col.ind=apply(....................))+\nscale_color_gradient2(low=\"white\", mid=\"blue\",high=\"red\", midpoint=0.8, space = \"Lab\")\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nfviz_pca_ind(resacp,axes=c(1,2),geom=c(\"point\"),habillage=as.factor(resfcm$cluster))\n\n\n\nfviz_pca_ind(resacp,axes=c(1,2),geom=c(\"point\"),col.ind=apply(resfcm$u,1,max))+\nscale_color_gradient2(low=\"white\", mid=\"blue\",high=\"red\", midpoint=0.8, space = \"Lab\")\n\n\n\n\n\n\nQuestion : Comparez les classifications obtenues avec Kmeans et fuzzy c-means. Commentez.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nreskmeans&lt;-kmeans(wine[,-c(1,2)],3)\nadjustedRandIndex(reskmeans$cluster,resfcm$cluster)\n\n[1] 0.8800417\n\ntable(reskmeans$cluster,resfcm$cluster)\n\n   \n      1   2   3\n  1 228   9  11\n  2   0   0 182\n  3   2 166   2\n\n\n\nset.seed(1234)\nlibrary(circlize)\n#library(viridis)\nlibrary(reshape2)\nclust1F&lt;-paste(\"Cl1-K\",reskmeans$cluster,sep=\"\")\nclust2F&lt;-paste(\"Cl2-K\",resfcm$cluster,sep=\"\")\n#Tab&lt;-melt(table(clust1F,clust2F))\n#mycolor &lt;- viridis(6, alpha = 1, begin = 0, end = 1, option = \"H\")\n#mycolor &lt;- mycolor[sample(1:6)]\nchordDiagram(table(clust2F,clust1F)) #,grid.col=mycolor)"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#statistiques-descriptives-1",
    "href": "TP1-Kmeans-Correction.html#statistiques-descriptives-1",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "2.1 Statistiques descriptives",
    "text": "2.1 Statistiques descriptives\nQuestion : Commencez par charger le jeu de données zoo-dataTP.txt et faites quelques statistiques descriptives pour vous familiariser avec ce jeu de données.\n\nzoo&lt;- ...\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\n# Trace de construction des données\nzoo&lt;-read.table(\"zoo.data\",sep=\",\")\nfor (j in 2:ncol(zoo))\n  zoo[,j]&lt;-as.factor(zoo[,j])\nzoo[27,1]&lt;-\"frog1\"\nrownames(zoo)&lt;- zoo[,1]\ntype&lt;-zoo[,18]\nzoo&lt;-zoo[,-c(1,18)]\ncolnames(zoo)&lt;-c(\"hair\",\"feathers\",\"eggs\",\"milk\",\"airbone\",\"aquatic\",\"predator\",\"toothed\",\"backbone\",\"breathes\",\"venomous\",\"fins\",\"legs\",\"tail\",\"domestic\",\"catsize\")\nwrite.table(zoo,file=\"zoo-dataTP.txt\")\n\n\nzoo&lt;-read.table(\"zoo-dataTP.txt\",header=T,stringsAsFactors = TRUE)\nfor (j in 1:ncol(zoo))\n  zoo[,j]&lt;-as.factor(zoo[,j])\nsummary(zoo)\n\n hair   feathers eggs   milk   airbone aquatic predator toothed backbone\n 0:58   0:81     0:42   0:60   0:77    0:65    0:45     0:40    0:18    \n 1:43   1:20     1:59   1:41   1:24    1:36    1:56     1:61    1:83    \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n breathes venomous fins   legs   tail   domestic catsize\n 0:21     0:93     0:84   0:23   0:26   0:88     0:57   \n 1:80     1: 8     1:17   2:27   1:75   1:13     1:44   \n                          4:38                          \n                          5: 1                          \n                          6:10                          \n                          8: 2                          \n\ndim(zoo)\n\n[1] 101  16\n\n\n\n\nQuestion : Faites une analyse en composantes multiples de ce jeu de données.\n\nres.mca&lt;-MCA(....)\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\nres.mca&lt;- MCA(zoo,ncp = 5, graph = FALSE)\n\nfviz_screeplot(res.mca)\n\n\n\n#plot(res.mca, invisible = c(\"quali.sup\", \"ind\"), cex=1, col.var = \"darkblue\", cex.main=2, col.main= \"darkblue\")\nfviz_mca_ind(res.mca,col.ind=\"darkblue\",repel=T)\n\n\n\nfviz_mca_var(res.mca,col.var=\"darkblue\",repel=T)\n\n\n\nfviz_mca_var(res.mca, choice = \"mca.cor\", \n            repel = TRUE, \n            ggtheme = theme_minimal())\n\n\n\n\n\n### Contrib des variables\nfviz_contrib(res.mca, choice = \"var\", axes =1)\n\n\n\nfviz_contrib(res.mca, choice = \"var\", axes =2)\n\n\n\n\n\n### plot of individuals \nfviz_mca_ind(res.mca,  habillage=4,repel=T)\n\n\n\nfviz_mca_ind(res.mca,  habillage=3,repel=T)\n\n\n\nfviz_mca_ind(res.mca,  habillage=12,repel=T)\n\n\n\nfviz_mca_ind(res.mca,  habillage=13,repel=T)"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#fonctions-auxiliaires-pour-la-suite",
    "href": "TP1-Kmeans-Correction.html#fonctions-auxiliaires-pour-la-suite",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "2.2 Fonctions auxiliaires pour la suite",
    "text": "2.2 Fonctions auxiliaires pour la suite\nPour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction barplotClus() permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d’un clustering donné.\n\n# J indice des variables\n# Data = jeu de données\n# clust = clustering étudié\n# output : liste des graphes par variable dans J donnant la répartition des modalités de J par classe de clust\n\nbarplotClus &lt;- function(clust, Data, J) {\n    aux.long.p &lt;- heatm(clust, Data, J)$freq\n    p&lt;-NULL\n    \n    for (j in 1:length(J)) {\n        p[[j]] &lt;- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[J[j]]), ],\n            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = \"identity\")+\n            labs(fill = colnames(Data)[J[j]])\n    }\n    return(p)\n}\nheatm &lt;- function(clust, Data, J) {\n    library(dplyr)\n    Dataaux &lt;- data.frame(id.s = c(1:nrow(Data)), Data)\n    aux &lt;- cbind(Dataaux, clust)\n    aux.long &lt;- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,\n        id = c(\"id.s\", \"clust\"), factorsAsStrings = T)\n    # Effectifs\n    aux.long.q &lt;- aux.long %&gt;%\n        group_by(clust, variable, value) %&gt;%\n        mutate(count = n_distinct(id.s)) %&gt;%\n        distinct(clust, variable, value, count)\n    # avec fréquences\n    aux.long.p &lt;- aux.long.q %&gt;%\n        group_by(clust, variable) %&gt;%\n        mutate(perc = count/sum(count)) %&gt;%\n        arrange(clust)\n\n    Lev &lt;- NULL\n    for (j in 1:ncol(Data)) Lev &lt;- c(Lev, levels(Data[, j]))\n\n    Jaux &lt;- NULL\n    for (j in 1:length(J)) {\n        Jaux &lt;- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))\n    }\n\n    gaux &lt;- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +\n        scale_fill_gradient2(low = \"white\", mid = \"blue\", high = \"red\") + theme_minimal()\n\n    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))\n}"
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#clustering-à-laide-des-kmodes",
    "href": "TP1-Kmeans-Correction.html#clustering-à-laide-des-kmodes",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "2.3 Clustering à l’aide des Kmodes",
    "text": "2.3 Clustering à l’aide des Kmodes\nDans cette partie, nous allons utiliser la méthode des Kmodes introduite par Huang (1998). Rappelons que cette méthode est une extension des Kmeans dans le cas des données qualitatives. Les modifications par rapport aux Kmeans sont\n\nle changement de distance : on utilise la dissimilarité basée sur l’appariement simple\n\n\\[\nd(\\mathbf{x}_i,\\mathbf{x}_\\ell) = \\underset{j=1}{\\stackrel{p}{\\sum}}\\ \\mathbb{1}_{x_{ij}\\neq x_{\\ell j}}\n\\]\n\nle centre d’une classe est calculé en fonction des fréquences des modalités majoritaires présentes dans cette classe: pour la classe \\(\\mathcal{C}_k\\),\n\n\\[\n\\mathbf{m}_k=(m_{k1},\\ldots,m_{kp}) \\textrm{ avec } m_{kj}= \\underset{u_1,\\ldots,u_{s_j}}{\\mbox{argmax}}\\ \\underset{i\\in\\mathcal C_k}{\\sum}\\ \\mathbb{1}_{x_{ij}= u_{s_j}}\n\\]\nQuestion : A l’aide de la fonction kmodes()de la librairie klaR, déterminez une classification en \\(K=6\\) classes des animaux. Visualisez la classification obtenue. Vous pouvez vous aider des fonctions auxiliaires pour interpréter la classification.\n\nlibrary(klaR)\nreskmodes&lt;-kmodes(....)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nlibrary(klaR)\nK &lt;- 6\nclkmodes &lt;- kmodes(zoo, K, iter.max = 100, weight = FALSE)\ntable(clkmodes$cluster)\n\n\n 1  2  3  4  5  6 \n19 19 20 20  4 19 \n\n\n\nclkmodes$modes\n\n  hair feathers eggs milk airbone aquatic predator toothed backbone breathes\n1    0        0    1    0       0       0        0       0        0        1\n2    1        0    0    1       0       0        1       1        1        1\n3    1        0    0    1       0       0        0       1        1        1\n4    0        1    1    0       1       0        0       0        1        1\n5    0        0    0    1       0       1        1       1        1        1\n6    0        0    1    0       0       1        1       1        1        0\n  venomous fins legs tail domestic catsize\n1        0    0    6    0        0       0\n2        0    0    4    1        0       1\n3        0    0    4    1        0       1\n4        0    0    2    1        0       0\n5        0    1    0    1        0       1\n6        0    1    0    1        0       0\n\n\n\nfviz_mca_ind(res.mca, habillage = as.factor(clkmodes$cluster), geom = c(\"point\",\"text\"),repel=T) + \n  labs(title = \"Kmodes\")\n\n\n\n\n\np&lt;-barplotClus(clkmodes$cluster, zoo, J = c(1:4))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\nrm(p)\np&lt;-barplotClus(clkmodes$cluster, zoo, J = c(5:8))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\nrm(p)\np&lt;-barplotClus(clkmodes$cluster, zoo, J = c(9:12))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\nrm(p)\np&lt;-barplotClus(clkmodes$cluster, zoo, J = c(13:16))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\nrm(p)\n\n\ndf&lt;-data.frame(cl=as.factor(clkmodes$cluster),zoo)\nres.mcab&lt;- MCA(df,quali.sup=1,ncp = 5, graph = FALSE)\n\n\nfviz_mca_ind(res.mcab,col.ind=\"darkblue\",repel=T,habillage=1)\n\n\n\nfviz_mca_var(res.mcab,col.var=\"darkblue\",repel=T)\n\n\n\n\n\n\nQuestion : Pour déterminer le nombre de classes, la méthode du coude peut être utilisée en remplaçant l’inertie intra-classe par le critère “Within Cluster Difference”\n\\[\nWCD(K) = \\underset{k=1}{\\stackrel{K}{\\sum}} \\underset{i\\in\\mathcal C_k}{\\sum}\\ d(x_i,m_k)  \n\\]\noù \\(d(.,.)\\) est l’appariement simple et \\(m_k\\) est le centre de la classe \\(\\mathcal C_k\\).\nTracez la courbe \\(K\\mapsto WCD(K)\\) pour déterminer le nombre de classes optimal. Vous pouvez vous aider des sorties de la fonction kmodes().\n\n# A COMPLETER\nWithinDiff&lt;-NULL\nKmax&lt;-10\nClust&lt;-matrix(0,nrow=nrow(zoo),ncol=Kmax)\nfor (k in 1:Kmax){\n  aux&lt;-kmodes(.........)\n  WithinDiff&lt;-c(WithinDiff,..........)\n  Clust[,k]&lt;-aux$cluster\n}\n\nauxdf&lt;-data.frame(NbCluster=1:Kmax,WithinDiff=WithinDiff)\nggplot(auxdf,aes(x=NbCluster,y=WithinDiff))+geom_point()+\n  geom_line()\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nWithinDiff &lt;- NULL\nKmax &lt;- 10\nClust &lt;- matrix(0, nrow = nrow(zoo), ncol = Kmax)\nfor (k in 1:Kmax) {\n    aux &lt;- kmodes(zoo, k, iter.max = 100, weight = FALSE)\n    WithinDiff &lt;- c(WithinDiff, sum(aux$withindiff))\n    Clust[, k] &lt;- aux$cluster\n}\n\nauxdf &lt;- data.frame(NbCluster = 1:Kmax, WithinDiff = WithinDiff)\nggplot(auxdf, aes(x = NbCluster, y = WithinDiff)) + geom_point() + geom_line()\n\n\n\n\n\n\nQuestion Etudiez la classification retenue. On la notera clustkmodes pour la suite\n\n# A COMPLETER\nclustkmodes&lt;- ...\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nclustkmodes&lt;-Clust[,4]\ntable(clustkmodes)\n\nclustkmodes\n 1  2  3  4 \n22 39 19 21 \n\nfor (k in 1:4){\n  I&lt;-which(Clust[,4]==k)\n  print(paste(\"Classe \",k,sep=\"\"))\n  print(rownames(zoo)[I])\n}\n\n[1] \"Classe 1\"\n [1] \"bass\"     \"carp\"     \"catfish\"  \"chub\"     \"dogfish\"  \"dolphin\" \n [7] \"frog\"     \"frog1\"    \"haddock\"  \"herring\"  \"newt\"     \"pike\"    \n[13] \"piranha\"  \"pitviper\" \"porpoise\" \"seahorse\" \"seasnake\" \"slowworm\"\n[19] \"sole\"     \"stingray\" \"tuatara\"  \"tuna\"    \n[1] \"Classe 2\"\n [1] \"aardvark\" \"antelope\" \"bear\"     \"boar\"     \"buffalo\"  \"calf\"    \n [7] \"cavy\"     \"cheetah\"  \"deer\"     \"elephant\" \"fruitbat\" \"giraffe\" \n[13] \"girl\"     \"goat\"     \"gorilla\"  \"hamster\"  \"hare\"     \"leopard\" \n[19] \"lion\"     \"lynx\"     \"mink\"     \"mole\"     \"mongoose\" \"opossum\" \n[25] \"oryx\"     \"platypus\" \"polecat\"  \"pony\"     \"puma\"     \"pussycat\"\n[31] \"raccoon\"  \"reindeer\" \"seal\"     \"sealion\"  \"squirrel\" \"vampire\" \n[37] \"vole\"     \"wallaby\"  \"wolf\"    \n[1] \"Classe 3\"\n [1] \"clam\"     \"crab\"     \"crayfish\" \"flea\"     \"gnat\"     \"honeybee\"\n [7] \"housefly\" \"ladybird\" \"lobster\"  \"moth\"     \"octopus\"  \"scorpion\"\n[13] \"seawasp\"  \"slug\"     \"starfish\" \"termite\"  \"toad\"     \"wasp\"    \n[19] \"worm\"    \n[1] \"Classe 4\"\n [1] \"chicken\"  \"crow\"     \"dove\"     \"duck\"     \"flamingo\" \"gull\"    \n [7] \"hawk\"     \"kiwi\"     \"lark\"     \"ostrich\"  \"parakeet\" \"penguin\" \n[13] \"pheasant\" \"rhea\"     \"skimmer\"  \"skua\"     \"sparrow\"  \"swan\"    \n[19] \"tortoise\" \"vulture\"  \"wren\""
  },
  {
    "objectID": "TP1-Kmeans-Correction.html#clustering-avec-les-kmeans-sur-les-coordonnées-de-acm",
    "href": "TP1-Kmeans-Correction.html#clustering-avec-les-kmeans-sur-les-coordonnées-de-acm",
    "title": "TP 1 - Kmeans et ses variantes",
    "section": "2.4 Clustering avec les Kmeans sur les coordonnées de ACM",
    "text": "2.4 Clustering avec les Kmeans sur les coordonnées de ACM\nUne seconde stratégie est de partir des coordonnées de l’analyse des correspondances multiples (ACM) et d’utiliser un algorithme plus usuel sur données quantitatives. Dans cette section, on va appliquer l’algorithme des Kmeans.\nQuestion : Appliquez l’algorithme des Kmeans sur les coordonnées de l’ACM. Pour la détermination du nombre de classes, vous pouvez utiliser l’évolution de l’inertie intra-classe et le critère silhouette.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nset.seed(1234)\nKmax &lt;- 10\nkmeansclus &lt;- matrix(0, nrow = nrow(zoo), ncol = (Kmax - 1))\nIintra &lt;- NULL\nSilhou &lt;- NULL\nfor (k in 2:Kmax) {\n    resaux &lt;- kmeans(res.mca$ind$coord[, 1:5], centers = k, nstart = 10)\n    kmeansclus[, (k - 1)] &lt;- resaux$cluster\n    Iintra &lt;- c(Iintra, resaux$tot.withinss)\n    aux &lt;- silhouette(resaux$cluster, daisy(res.mca$ind$coord))\n    Silhou &lt;- c(Silhou, mean(aux[, 3]))\n}\n\ndf &lt;- data.frame(K = 2:Kmax, Iintra = Iintra, Silhou = Silhou)\ng1 &lt;- ggplot(df, aes(x = K, y = Iintra)) + geom_line() + geom_point() + xlab(\"Nombre de classes\") +\n    ylab(\"Inertie intraclasse\")\ng2 &lt;- ggplot(df, aes(x = K, y = Silhou)) + geom_line() + geom_point() + xlab(\"Nombre de classes\") +\n    ylab(\"Critère Silhouette\")\ngrid.arrange(g1, g2, ncol = 2)\n\n\n\n\n\n\nQuestion : Etudiez la classification retenue. Comparez avec la classification obtenue précédemment avec les Kmodes.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\n##Comparaison entre 4 et 6 classes pour Kmeans sur MCA\ntable(kmeansclus[,3],kmeansclus[,5])\n\n   \n     1  2  3  4  5  6\n  1  1 10  0  0  0  9\n  2  0  0 20  0  0  0\n  3  3  0  0 18  0  0\n  4 19  0  0  0 21  0\n\n## Comparaison entre Kmeans sur MCA et Kmodes\ntable(kmeansclus[,3],clustkmodes)\n\n   clustkmodes\n     1  2  3  4\n  1  1  0 19  0\n  2  0  0  0 20\n  3 20  1  0  0\n  4  1 38  0  1\n\nadjustedRandIndex(kmeansclus[,3],clustkmodes)\n\n[1] 0.8918999\n\nclust1F&lt;-paste(\"Cl-Kmeans\",kmeansclus[,3],sep=\"\")\nclust2F&lt;-paste(\"Cl-Kmodes\",clustkmodes,sep=\"\")\nchordDiagram(table(clust2F,clust1F))"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html",
    "href": "TP5-Velib-SujetEtudiant.html",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "",
    "text": "library(funFEM)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(corrplot)\nlibrary(mclust)\nlibrary(stringr)\nlibrary(cluster)\nlibrary(clusterSim)\nlibrary(leaflet)"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#préparation-des-données",
    "href": "TP5-Velib-SujetEtudiant.html#préparation-des-données",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "1.1 Préparation des données",
    "text": "1.1 Préparation des données\nCet ensemble de données contient des données provenant du système de partage de vélos de Paris, appelé Vélib. Les données sont des profils de chargement des stations de vélos sur une semaine. Les données ont été collectées toutes les heures pendant la période du dimanche 1er septembre au dimanche 7 septembre 2014.\nOn charge ici les données Velib disponibles dans la librairie funFEM.\n\nlibrary(funFEM)\ndata(velib)\n\nCes données contiennent\n\ndata : les profils de chargement (nb de vélos disponibles / nb de points d’attache) des 1189 stations à 181 points de temps.\nposition : la longitude et la latitude des 1189 stations de vélos.\ndates : les dates de téléchargement.\nbonus : indique si la station est sur une colline (bonus = 1).\nnames : les noms des stations.\n\nAfin d’avoir des semaines complètes, nous allons supprimer les 13 premières colonnes.\n\nVelibdata&lt;-velib$data[,-c(1:13)]\ncolnames(Velibdata)&lt;-velib$dates[-c(1:13)]\n\nOn controle le nombre de jours et d’heures dans le jeu de données\n\nday&lt;-str_sub(colnames(Velibdata), 1, 3)\nday&lt;-factor(day, levels = c(\"Lun\",\"Mar\",\"Mer\",\"Jeu\",\"Ven\",\"Sam\",\"Dim\"))\ntable(day)\n\nday\nLun Mar Mer Jeu Ven Sam Dim \n 24  24  24  24  24  24  24 \n\nhour&lt;-str_sub(colnames(Velibdata),5,6)\ntable(hour)\n\nhour\n00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n 7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7 \n\ntimeTick &lt;- 24*(0:6)  \n\nOn charge des informations sur les stations de Velib disponibles dans le fichier InfoStationsVelib.txt.\n\nstation&lt;- read.table(\"InfoStationsVelib.txt\",header=T)"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#quelques-fonctions-auxiliaires-pour-exploiter-les-résultats",
    "href": "TP5-Velib-SujetEtudiant.html#quelques-fonctions-auxiliaires-pour-exploiter-les-résultats",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "1.2 Quelques fonctions auxiliaires pour exploiter les résultats",
    "text": "1.2 Quelques fonctions auxiliaires pour exploiter les résultats\nFonction pour tracer les profils (charge de chaque station / loading) en fonction des jours et heures pour les stations choisies.\n\nplotprofils&lt;-function(Velibdata,station,numstation,plotsem=TRUE){\n  # Velibdata = les données de velib initiales\n  # station = ens. des informations sur les stations\n  # numstation = vecteur contenant le numéro de ligne des stations dont on souhaite tracer les profils\n  \nlibrary(reshape2)\nlibrary(ggplot2)\nDataaux&lt;-data.frame(id.s=station$nom,Velibdata)\nAux&lt;-melt(Dataaux[numstation,],id = c(\"id.s\"))\nAux&lt;-data.frame(Aux,day=str_sub(Aux$variable,1,3),hour=as.numeric(str_sub(Aux$variable,5,6)))\nAux$day&lt;-factor(Aux$day, levels = c(\"Lun\",\"Mer\",\"Mar\",\"Sam\",\"Jeu\",\"Dim\",\"Ven\"))\n\nif (plotsem==TRUE){\ng&lt;-ggplot(Aux,aes(x=hour,y=value,col=id.s))+\n  geom_line()+\n  facet_wrap(~day,ncol=2)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  scale_x_continuous(breaks=seq(0,24,4))+\n  theme(legend.position = \"bottom\")+theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=c(0,7,9,12,14,20,24))\n}else{\nv&lt;-rep(0,nrow(Aux))\nfor (j in 1:length(levels(Aux$day)))\n  v[which(Aux$day==levels(Aux$day)[j])]&lt;-Aux$hour[which(Aux$day==levels(Aux$day)[j])] + (24*(j-1))\nAux&lt;-data.frame(Aux,v=v)\nrect&lt;-data.frame(xmin=timeTick,xmax=timeTick+23,ymin=rep(0,7),ymax=rep(1,7),color=rainbow(n=7))\ng&lt;-ggplot(data=Aux)+\n  geom_line(data=Aux,aes(x=v,y=value,col=id.s))+\n  geom_rect(data=rect,aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill=color),show.legend=FALSE,alpha=0.1)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=rep(seq(0,21,3),7) + rep(24*(0:6),each=4),labels = rep(seq(0,21,3),7))  \n}\nreturn(g)\n}\n\nExemple d’utilisation :\n\nplotprofils(Velibdata,station,numstation=c(813,655,468))\n\n\n\nplotprofils(Velibdata,station,numstation=c(813,655,468),plotsem=FALSE)\n\n\n\n\nSe donnant une classification, fonction pour tracer les profils moyens par classe :\n\nplotprofilsmoy&lt;-function(Velibdata,clustering,plotsem=TRUE){\nlibrary(reshape2)\nlibrary(ggplot2)\n  \nDataaux&lt;-matrix(0,nrow=max(clustering),ncol=ncol(Velibdata))\nfor (k in 1:max(clustering)){\n  Dataaux[k,]&lt;-apply(Velibdata[which(clustering==k),],2,mean)\n}\ncolnames(Dataaux)&lt;-colnames(Velibdata)\nDataaux&lt;-data.frame(id.s=as.factor(1:max(clustering)),Dataaux)\nAux&lt;-melt(Dataaux,id = c(\"id.s\"))\nAux&lt;-data.frame(Aux,day=str_sub(Aux$variable,1,3),hour=as.numeric(str_sub(Aux$variable,5,6)))\nAux$day&lt;-factor(Aux$day, levels = c(\"Lun\",\"Mer\",\"Mar\",\"Sam\",\"Jeu\",\"Dim\",\"Ven\"))\n\nif(plotsem==TRUE){\ng&lt;-ggplot(Aux,aes(x=hour,y=value,col=id.s))+\n  geom_line()+\n  facet_wrap(~day,ncol=2)+xlab(\"Time\")+ylab(\"Loading\")+\n  ylim(0,1)+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=c(0,7,9,12,14,20,24))\n}\nelse{\nv&lt;-rep(0,nrow(Aux))\nfor (j in 1:length(levels(Aux$day)))\n  v[which(Aux$day==levels(Aux$day)[j])]&lt;-Aux$hour[which(Aux$day==levels(Aux$day)[j])] + (24*(j-1))\nAux&lt;-data.frame(Aux,v=v)\nrect&lt;-data.frame(xmin=timeTick,xmax=timeTick+23,ymin=rep(0,7),ymax=rep(1,7),color=rainbow(n=7))  \ng&lt;-ggplot(data=Aux)+\n  geom_line(data=Aux,aes(x=v,y=value,col=id.s))+\n  geom_rect(data=rect,aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill=color),show.legend=FALSE,alpha=0.1)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  ylim(0,1)+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=rep(seq(0,21,3),7) + rep(24*(0:6),each=4),labels = rep(seq(0,21,3),7))\n}\nreturn(g)\n}\n\nExemple d’utilisation avec les stations sur colline par rapport aux autres :\n\nplotprofilsmoy(Velibdata,clustering=station$colline+1,plotsem=TRUE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=station$colline+1,plotsem=FALSE)\n\n\n\n\nFonction auxiliaire pour comparer une classification avec les informations auxiliaires disponibles pour les stations\n\nstationcaract&lt;-function(station,clustering){\n  library(dplyr)\n  #Dataaux&lt;-data.frame(id.s=c(1:nrow(Data)),station)\n  #aux &lt;- cbind(Dataaux, clust)\n  aux&lt;-cbind(station,clustering)\n  aux.long &lt;- melt(data.frame(lapply(aux[,-c(2:3)],as.character)),stringsAsFactors=FALSE, id = c(\"nom\", \"clustering\"), factorsAsStrings=T)\n  # Effectifs\n  aux.long.q &lt;- aux.long %&gt;%\n    group_by(clustering, variable, value) %&gt;%\n    mutate(count = n_distinct(nom)) %&gt;%\n    distinct(clustering, variable, value, count)\n  # avec fréquences\n  aux.long.p &lt;- aux.long.q %&gt;%\n    group_by(clustering, variable) %&gt;%\n    mutate(perc = count / sum(count)) %&gt;%\n    arrange(clustering)\n\ngaux&lt;-ggplot(data=aux.long.p, aes(x=clustering, y=perc, fill=value)) +\n  geom_bar(stat=\"identity\")+facet_wrap(~variable)\n\nreturn(gaux)\n}\n\nFonction pour observer une variable quantitative sur une carte 2D de Paris\n\nplotmapquanti&lt;-function(dataposition,varquanti){\n  library(leaflet)\n  pal &lt;- colorNumeric(palette = \"RdYlBu\",domain = varquanti)\n  leaflet(dataposition) %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(radius = 3,color = pal(varquanti),stroke = FALSE, fillOpacity = 1)%&gt;%\n  addLegend(\"bottomright\", pal = pal, values = varquanti,\n    opacity = 1)\n}\n\nPar exemple, la charge moyenne par station :\n\nplotmapquanti(dataposition=velib$position,varquanti=rowMeans(Velibdata))\n\n\n\n\n\nFonction pour observer une variable qualitative sur une carte 2D de Paris :\n\nplotmapquali&lt;-function(dataposition,varquali){\nlibrary(leaflet)\nfactpal &lt;- colorFactor(topo.colors(nlevels(varquali)), varquali)\n\nleaflet(dataposition) %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(radius = 3,color = factpal(varquali),stroke = FALSE, fillOpacity = 0.9)%&gt;%\n  addLegend(\"bottomright\", pal = factpal, values = varquali, opacity = 1)\n}\n\nPar exemple avec la variable binaire colline:\n\nplotmapquali(dataposition=velib$position,varquali=as.factor(station$colline))"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#quelques-statistiques-descriptives",
    "href": "TP5-Velib-SujetEtudiant.html#quelques-statistiques-descriptives",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "1.3 Quelques statistiques descriptives",
    "text": "1.3 Quelques statistiques descriptives\nOn reprend ici rapidement quelques éléments d’analyse descriptive vus précédemment dans l’UF.\n\nTracé des corrélations des heures par jour, toutes les stations confondues :\n\n\nlibrary(corrplot)\ntimeTickAux&lt;-c(timeTick,168)\nfor (k in 1:7)\n  corrplot(cor(Velibdata[,(timeTickAux[k]+1):(timeTickAux[k+1])]),method=\"ellipse\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComportement moyen par jour :\n\n\nvelibmeanday&lt;-matrix(0,nrow=nrow(Velibdata),ncol=7)\nfor (k in 1:7){\nvelibmeanday[,k] &lt;- apply(Velibdata[,(timeTickAux[k]+1):(timeTickAux[k+1])],1,mean)\n}  \ncolnames(velibmeanday)&lt;-c(\"Lun\",\"Mar\",\"Mer\",\"Jeu\",\"Ven\",\"Sam\",\"Dim\")\nggplot(melt(velibmeanday),aes(x=Var2,y=value))+geom_boxplot()+xlab(\"\")+ylab(\"Loading\")\n\n\n\n\n\nComportement moyen par heure et par jour :\n\n\nvelibHour &lt;- data.frame(value=colMeans(Velibdata),day=day,hour=as.numeric(hour))\nggplot(velibHour,aes(x=hour,y=value,col=day))+\n  geom_line()+geom_point()+\n  ylab(\"Loading\")+ggtitle(\"Hourly loading, averaged over all stations\")\n\n\n\n\n\nQuelques visualisations des stations sur le plan 2D de Paris :\n\n\nplotmapquanti(velib$position,rowMeans(Velibdata))\n\n\n\n\n\n\nt&lt;-0\nloadheure  &lt;- rowMeans(Velibdata[, seq(from = (t+1),  by = 24, length = 7)])\nplotmapquanti(velib$position,loadheure)\n\n\n\n\n\n\nVisualisation des stations selon les informations auxiliaires sur les stations :\n\n\nplotmapquali(velib$position,as.factor(station$colline))\n\n\n\n\n\n\nplotmapquali(velib$position,as.factor(station$tourisme))"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#analyse-en-composantes-principales",
    "href": "TP5-Velib-SujetEtudiant.html#analyse-en-composantes-principales",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "1.4 Analyse en composantes principales",
    "text": "1.4 Analyse en composantes principales\nQuestion :  Faites une ACP des stations et analysez les résultats.\n\nrespca&lt;-PCA(.......)\nfviz_eig(....)\nfviz_pca_var(......)\nfviz_pca_ind(........)"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#clustering-avec-lalgorithme-des-kmeans",
    "href": "TP5-Velib-SujetEtudiant.html#clustering-avec-lalgorithme-des-kmeans",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "2.1 Clustering avec l’algorithme des Kmeans",
    "text": "2.1 Clustering avec l’algorithme des Kmeans\nQuestion : A l’aide d’une procédure des Kmeans, déterminez une classification des stations.\n\n# A COMPLETER\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue."
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#clustering-avec-de-la-classification-hiérarchique",
    "href": "TP5-Velib-SujetEtudiant.html#clustering-avec-de-la-classification-hiérarchique",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "2.2 Clustering avec de la classification hiérarchique",
    "text": "2.2 Clustering avec de la classification hiérarchique\nQuestion : A l’aide d’une procédure hiérarchique, déterminez une classification des stations.\n\n# A COMPLETER\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue par CAH. Comparez cette classification avec celle obtenue par les Kmeans.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#clustering-avec-des-mélanges-gaussiens",
    "href": "TP5-Velib-SujetEtudiant.html#clustering-avec-des-mélanges-gaussiens",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "2.3 Clustering avec des mélanges gaussiens",
    "text": "2.3 Clustering avec des mélanges gaussiens\nQuestion : A l’aide des mélanges gaussiens, déterminez une classification des stations.\n\n# A COMPLETER\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue par modèles de mélange. Comparez cette classification avec celles obtenues précédemment.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#préparation-des-données-1",
    "href": "TP5-Velib-SujetEtudiant.html#préparation-des-données-1",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "3.1 Préparation des données",
    "text": "3.1 Préparation des données\n\nVelibMean&lt;-matrix(0,nrow=nrow(Velibdata),ncol=3*7)\nfor (jour in 1:7){\n  VelibMean[,3*(jour-1)+1]&lt;-apply(Velibdata[,24*(jour-1)+c(1:8)],1,mean)\n  VelibMean[,3*(jour-1)+2]&lt;-apply(Velibdata[,24*(jour-1)+c(9:21)],1,mean)\n  VelibMean[,3*(jour-1)+3]&lt;-apply(Velibdata[,24*(jour-1)+c(22:24)],1,mean)\n}\ncolnames(VelibMean)&lt;-paste(rep(unique(day),each=3),\"-\",rep(c(\"0-7h\",\"8h-20h\",\"21-23h\"),7),sep=\"\")\n\nQuestion : Etudiez les corrélations entre variables du jeu de données VelibMean.\n\n# A COMPLETER\n\nQuestion : Faites une ACP et commentez.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP5-Velib-SujetEtudiant.html#méthodes-de-classification",
    "href": "TP5-Velib-SujetEtudiant.html#méthodes-de-classification",
    "title": "TP 6 : Clustering des données Vélib",
    "section": "3.2 Méthodes de classification",
    "text": "3.2 Méthodes de classification\n\n3.2.1 Kmeans\nQuestion : Faites une classification des stations à partir du jeu de données VelibMean à l’aide des Kmeans. Etudiez la classification obtenue.\n\n# A COMPLETER\n\n\n\n3.2.2 Mélanges gaussiens\nQuestion : Faites une classification des stations à partir du jeu de données VelibMean à l’aide des mélanges gaussiens. Etudiez la classification obtenue.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP5-Velib-Correction.html",
    "href": "TP5-Velib-Correction.html",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "",
    "text": "library(funFEM)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(corrplot)\nlibrary(mclust)\nlibrary(stringr)\nlibrary(cluster)\nlibrary(clusterSim)\nlibrary(leaflet)"
  },
  {
    "objectID": "TP5-Velib-Correction.html#préparation-des-données",
    "href": "TP5-Velib-Correction.html#préparation-des-données",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "1.1 Préparation des données",
    "text": "1.1 Préparation des données\nCet ensemble de données contient des données provenant du système de partage de vélos de Paris, appelé Vélib. Les données sont des profils de chargement des stations de vélos sur une semaine. Les données ont été collectées toutes les heures pendant la période du dimanche 1er septembre au dimanche 7 septembre 2014.\nOn charge ici les données Velib disponibles dans la librairie funFEM.\n\nlibrary(funFEM)\ndata(velib)\n\nCes données contiennent\n\ndata : les profils de chargement (nb de vélos disponibles / nb de points d’attache) des 1189 stations à 181 points de temps.\nposition : la longitude et la latitude des 1189 stations de vélos.\ndates : les dates de téléchargement.\nbonus : indique si la station est sur une colline (bonus = 1).\nnames : les noms des stations.\n\nAfin d’avoir des semaines complètes, nous allons supprimer les 13 premières colonnes.\n\nVelibdata&lt;-velib$data[,-c(1:13)]\ncolnames(Velibdata)&lt;-velib$dates[-c(1:13)]\n\nOn contrôle le nombre de jours et d’heures dans le jeu de données\n\nday&lt;-str_sub(colnames(Velibdata), 1, 3)\nday&lt;-factor(day, levels = c(\"Lun\",\"Mar\",\"Mer\",\"Jeu\",\"Ven\",\"Sam\",\"Dim\"))\ntable(day)\n\nday\nLun Mar Mer Jeu Ven Sam Dim \n 24  24  24  24  24  24  24 \n\nhour&lt;-str_sub(colnames(Velibdata),5,6)\ntable(hour)\n\nhour\n00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n 7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7 \n\ntimeTick &lt;- 24*(0:6)  \n\nOn charge des informations sur les stations de Velib disponibles dans le fichier InfoStationsVelib.txt.\n\nstation&lt;- read.table(\"InfoStationsVelib.txt\",header=T)"
  },
  {
    "objectID": "TP5-Velib-Correction.html#quelques-fonctions-auxiliaires-pour-exploiter-les-résultats",
    "href": "TP5-Velib-Correction.html#quelques-fonctions-auxiliaires-pour-exploiter-les-résultats",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "1.2 Quelques fonctions auxiliaires pour exploiter les résultats",
    "text": "1.2 Quelques fonctions auxiliaires pour exploiter les résultats\nFonction pour tracer les profils (charge de chaque station / loading) en fonction des jours et heures pour les stations choisies.\n\nplotprofils&lt;-function(Velibdata,station,numstation,plotsem=TRUE){\n  # Velibdata = les données de velib initiales\n  # station = ens. des informations sur les stations\n  # numstation = vecteur contenant le numéro de ligne des stations dont on souhaite tracer les profils\n  \nlibrary(reshape2)\nlibrary(ggplot2)\nDataaux&lt;-data.frame(id.s=station$nom,Velibdata)\nAux&lt;-melt(Dataaux[numstation,],id = c(\"id.s\"))\nAux&lt;-data.frame(Aux,day=str_sub(Aux$variable,1,3),hour=as.numeric(str_sub(Aux$variable,5,6)))\nAux$day&lt;-factor(Aux$day, levels = c(\"Lun\",\"Mer\",\"Mar\",\"Sam\",\"Jeu\",\"Dim\",\"Ven\"))\n\nif (plotsem==TRUE){\ng&lt;-ggplot(Aux,aes(x=hour,y=value,col=id.s))+\n  geom_line()+\n  facet_wrap(~day,ncol=2)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  scale_x_continuous(breaks=seq(0,24,4))+\n  theme(legend.position = \"bottom\")+theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=c(0,7,9,12,14,20,24))\n}else{\nv&lt;-rep(0,nrow(Aux))\nfor (j in 1:length(levels(Aux$day)))\n  v[which(Aux$day==levels(Aux$day)[j])]&lt;-Aux$hour[which(Aux$day==levels(Aux$day)[j])] + (24*(j-1))\nAux&lt;-data.frame(Aux,v=v)\nrect&lt;-data.frame(xmin=timeTick,xmax=timeTick+23,ymin=rep(0,7),ymax=rep(1,7),color=rainbow(n=7))\ng&lt;-ggplot(data=Aux)+\n  geom_line(data=Aux,aes(x=v,y=value,col=id.s))+\n  geom_rect(data=rect,aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill=color),show.legend=FALSE,alpha=0.1)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=rep(seq(0,21,3),7) + rep(24*(0:6),each=4),labels = rep(seq(0,21,3),7))  \n}\nreturn(g)\n}\n\nExemple d’utilisation :\n\nplotprofils(Velibdata,station,numstation=c(813,655,468))\n\n\n\nplotprofils(Velibdata,station,numstation=c(813,655,468),plotsem=FALSE)\n\n\n\n\nSe donnant une classification, fonction pour tracer les profils moyens par classe :\n\nplotprofilsmoy&lt;-function(Velibdata,clustering,plotsem=TRUE){\nlibrary(reshape2)\nlibrary(ggplot2)\n  \nDataaux&lt;-matrix(0,nrow=max(clustering),ncol=ncol(Velibdata))\nfor (k in 1:max(clustering)){\n  Dataaux[k,]&lt;-apply(Velibdata[which(clustering==k),],2,mean)\n}\ncolnames(Dataaux)&lt;-colnames(Velibdata)\nDataaux&lt;-data.frame(id.s=as.factor(1:max(clustering)),Dataaux)\nAux&lt;-melt(Dataaux,id = c(\"id.s\"))\nAux&lt;-data.frame(Aux,day=str_sub(Aux$variable,1,3),hour=as.numeric(str_sub(Aux$variable,5,6)))\nAux$day&lt;-factor(Aux$day, levels = c(\"Lun\",\"Mer\",\"Mar\",\"Sam\",\"Jeu\",\"Dim\",\"Ven\"))\n\nif(plotsem==TRUE){\ng&lt;-ggplot(Aux,aes(x=hour,y=value,col=id.s))+\n  geom_line()+\n  facet_wrap(~day,ncol=2)+xlab(\"Time\")+ylab(\"Loading\")+\n  ylim(0,1)+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=c(0,7,9,12,14,20,24))\n}\nelse{\nv&lt;-rep(0,nrow(Aux))\nfor (j in 1:length(levels(Aux$day)))\n  v[which(Aux$day==levels(Aux$day)[j])]&lt;-Aux$hour[which(Aux$day==levels(Aux$day)[j])] + (24*(j-1))\nAux&lt;-data.frame(Aux,v=v)\nrect&lt;-data.frame(xmin=timeTick,xmax=timeTick+23,ymin=rep(0,7),ymax=rep(1,7),color=rainbow(n=7))  \ng&lt;-ggplot(data=Aux)+\n  geom_line(data=Aux,aes(x=v,y=value,col=id.s))+\n  geom_rect(data=rect,aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill=color),show.legend=FALSE,alpha=0.1)+\n  xlab(\"Time\")+ylab(\"Loading\")+\n  ylim(0,1)+\n  theme(legend.position = \"bottom\")+\n  theme(axis.title.x = element_blank())+\n  labs(col = \"Classif.\")+\n  scale_x_continuous(breaks=rep(seq(0,21,3),7) + rep(24*(0:6),each=4),labels = rep(seq(0,21,3),7))\n}\nreturn(g)\n}\n\nExemple d’utilisation avec les stations sur colline par rapport aux autres :\n\nplotprofilsmoy(Velibdata,clustering=station$colline+1,plotsem=TRUE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=station$colline+1,plotsem=FALSE)\n\n\n\n\nFonction auxiliaire pour comparer une classification avec les informations auxiliaires disponibles pour les stations\n\nstationcaract&lt;-function(station,clustering){\n  library(dplyr)\n  #Dataaux&lt;-data.frame(id.s=c(1:nrow(Data)),station)\n  #aux &lt;- cbind(Dataaux, clust)\n  aux&lt;-cbind(station,clustering)\n  aux.long &lt;- melt(data.frame(lapply(aux[,-c(2:3)],as.character)),stringsAsFactors=FALSE, id = c(\"nom\", \"clustering\"), factorsAsStrings=T)\n  # Effectifs\n  aux.long.q &lt;- aux.long %&gt;%\n    group_by(clustering, variable, value) %&gt;%\n    mutate(count = n_distinct(nom)) %&gt;%\n    distinct(clustering, variable, value, count)\n  # avec fréquences\n  aux.long.p &lt;- aux.long.q %&gt;%\n    group_by(clustering, variable) %&gt;%\n    mutate(perc = count / sum(count)) %&gt;%\n    arrange(clustering)\n\ngaux&lt;-ggplot(data=aux.long.p, aes(x=clustering, y=perc, fill=value)) +\n  geom_bar(stat=\"identity\")+facet_wrap(~variable)\n\nreturn(gaux)\n}\n\nFonction pour observer une variable quantitative sur une carte 2D de Paris\n\nplotmapquanti&lt;-function(dataposition,varquanti){\n  library(leaflet)\n  pal &lt;- colorNumeric(palette = \"RdYlBu\",domain = varquanti)\n  leaflet(dataposition) %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(radius = 3,color = pal(varquanti),stroke = FALSE, fillOpacity = 1)%&gt;%\n  addLegend(\"bottomright\", pal = pal, values = varquanti,\n    opacity = 1)\n}\n\nPar exemple, la charge moyenne par station :\n\nplotmapquanti(dataposition=velib$position,varquanti=rowMeans(Velibdata))\n\n\n\n\n\nFonction pour observer une variable qualitative sur une carte 2D de Paris :\n\nplotmapquali&lt;-function(dataposition,varquali){\nlibrary(leaflet)\nfactpal &lt;- colorFactor(topo.colors(nlevels(varquali)), varquali)\n\nleaflet(dataposition) %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(radius = 3,color = factpal(varquali),stroke = FALSE, fillOpacity = 0.9)%&gt;%\n  addLegend(\"bottomright\", pal = factpal, values = varquali, opacity = 1)\n}\n\nPar exemple avec la variable binaire colline:\n\nplotmapquali(dataposition=velib$position,varquali=as.factor(station$colline))"
  },
  {
    "objectID": "TP5-Velib-Correction.html#quelques-statistiques-descriptives",
    "href": "TP5-Velib-Correction.html#quelques-statistiques-descriptives",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "1.3 Quelques statistiques descriptives",
    "text": "1.3 Quelques statistiques descriptives\nOn reprend ici rapidement quelques éléments d’analyse descriptive vus précédemment dans l’UF.\n\nTracé des corrélations des heures par jour, toutes les stations confondues :\n\n\nlibrary(corrplot)\ntimeTickAux&lt;-c(timeTick,168)\nfor (k in 1:7)\n  corrplot(cor(Velibdata[,(timeTickAux[k]+1):(timeTickAux[k+1])]),method=\"ellipse\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComportement moyen par jour :\n\n\nvelibmeanday&lt;-matrix(0,nrow=nrow(Velibdata),ncol=7)\nfor (k in 1:7){\nvelibmeanday[,k] &lt;- apply(Velibdata[,(timeTickAux[k]+1):(timeTickAux[k+1])],1,mean)\n}  \ncolnames(velibmeanday)&lt;-c(\"Lun\",\"Mar\",\"Mer\",\"Jeu\",\"Ven\",\"Sam\",\"Dim\")\nggplot(melt(velibmeanday),aes(x=Var2,y=value))+geom_boxplot()+xlab(\"\")+ylab(\"Loading\")\n\n\n\n\n\nComportement moyen par heure et par jour :\n\n\nvelibHour &lt;- data.frame(value=colMeans(Velibdata),day=day,hour=as.numeric(hour))\nggplot(velibHour,aes(x=hour,y=value,col=day))+\n  geom_line()+geom_point()+\n  ylab(\"Loading\")+ggtitle(\"Hourly loading, averaged over all stations\")\n\n\n\n\n\nQuelques visualisations des stations sur le plan 2D de Paris :\n\n\nplotmapquanti(velib$position,rowMeans(Velibdata))\n\n\n\n\n\n\nt&lt;-0\nloadheure  &lt;- rowMeans(Velibdata[, seq(from = (t+1),  by = 24, length = 7)])\nplotmapquanti(velib$position,loadheure)\n\n\n\n\n\n\nVisualisation des stations selon les informations auxiliaires sur les stations :\n\n\nplotmapquali(velib$position,as.factor(station$colline))\n\n\n\n\n\n\nplotmapquali(velib$position,as.factor(station$tourisme))"
  },
  {
    "objectID": "TP5-Velib-Correction.html#analyse-en-composantes-principales",
    "href": "TP5-Velib-Correction.html#analyse-en-composantes-principales",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "1.4 Analyse en composantes principales",
    "text": "1.4 Analyse en composantes principales\nQuestion :  Faites une ACP des stations et analysez les résultats.\n\nrespca&lt;-PCA(.......)\nfviz_eig(....)\nfviz_pca_var(......)\nfviz_pca_ind(........)\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nrespca &lt;-PCA(Velibdata,scale.unit = T,graph = F, ncp = 15)\n\n# Variance expliquée\nfviz_eig(respca)\n\n\n\nggplot(data.frame(comp=1:20,cumul=respca$eig[1:20,3]),aes(x=comp,y=cumul))+\n  geom_line()+geom_point()\n\n\n\n# Visualisation des variables\nfviz_pca_var(respca,axes=c(1,2),geom=c(\"arrow\"),col.var=day,palette=\"ucscgb\")\n\n\n\nfviz_pca_var(respca,axes=c(1,2),geom=c(\"arrow\"),col.var=hour,palette=\"ucscgb\")\n\n\n\nfor (k in 1:7)\n  corrplot(respca$var$cor[(timeTickAux[k]+1):(timeTickAux[k+1]),1:3],method=\"ellipse\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Projection des individus\nfviz_pca_ind(respca,axes=c(1,2),geom = c(\"point\"),habillage=as.factor(station$colline))\n\n\n\nfviz_pca_ind(respca,axes=c(1,2),geom = c(\"point\"),habillage=as.factor(station$tourisme))\n\n\n\n\n\nnbdim&lt;-4\naux&lt;-melt(respca$var$coord[,1:nbdim])\naux$Var1&lt;-str_sub(aux$Var1,1,3)\naux&lt;-data.frame(aux,time=rep(c(0:167),nbdim))\n\nrect&lt;-data.frame(xmin=timeTick,xmax=timeTick+23,ymin=rep(min(aux$value),7),ymax=rep(max(aux$value),7),color=rainbow(n=7))\n  \nggplot(data=aux)+\n  geom_rect(data=rect,aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill=color),alpha=0.3)+\n  geom_line(data=aux,aes(x=time,y=value))+\n  facet_wrap(~Var2,ncol=2)+\n  geom_hline(yintercept = 0,col=\"red\")+xlab(\"\")+ylab(\"\")+\n  scale_x_continuous(breaks=seq(0,168,24))+theme(legend.position = \"none\")"
  },
  {
    "objectID": "TP5-Velib-Correction.html#clustering-avec-lalgorithme-des-kmeans",
    "href": "TP5-Velib-Correction.html#clustering-avec-lalgorithme-des-kmeans",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "2.1 Clustering avec l’algorithme des Kmeans",
    "text": "2.1 Clustering avec l’algorithme des Kmeans\nQuestion : A l’aide d’une procédure des Kmeans, déterminez une classification des stations.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nset.seed(12345)\nKmax&lt;-20\nreskmeans&lt;-matrix(0,nrow=nrow(velibacp),ncol=(Kmax-1))\nIintra&lt;-NULL\nSilhou&lt;-NULL\nfor (k in 2:Kmax){\n  resaux&lt;-kmeans(velibacp,k,nstart = 20,iter.max=30)\n  reskmeans[,(k-1)]&lt;-resaux$cluster\n  Iintra&lt;-c(Iintra,resaux$tot.withinss)\n  aux&lt;-silhouette(resaux$cluster, daisy(velibacp))\n  Silhou&lt;-c(Silhou,mean(aux[,3]))\n}\n\nrm(resaux,aux)\n\ndf&lt;-data.frame(K=2:Kmax,Iintra=Iintra,Silhou=Silhou)\ng1&lt;-ggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()+xlab(\"Nombre de classes\")+ylab(\"Inertie intraclasse\")\ng2&lt;-ggplot(df,aes(x=K,y=Silhou))+geom_line()+geom_point()+xlab(\"Nombre de classes\")+ylab(\"Critère Silhouette\")\ngrid.arrange(g1,g2,ncol=2)\n\n\n\n\n\nresICL&lt;-mclustICL(velibacp,G=1:20,modelNames=c(\"EII\"))\nsummary(resICL)\n\nBest ICL values:\n            EII,11      EII,13       EII,19\nICL      -41830.01 -41855.1410 -41862.86314\nICL diff      0.00    -25.1267    -32.84888\n\n\n\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\nOn retient la classification en \\(K=5\\) classes :\n\nKkmeans&lt;-5\nclassifkmeans&lt;-reskmeans[,Kkmeans-1]\ntable(classifkmeans)\n\nclassifkmeans\n  1   2   3   4   5 \n406 151 209 241 182 \n\n\nOn visualise la classification sur la carte 2D :\n\nplotmapquali(velib$position,as.factor(classifkmeans))\n\n\n\n\n\nProfil moyen par classe :\n\nplotprofilsmoy(Velibdata,clustering=classifkmeans,plotsem=TRUE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=classifkmeans,plotsem=FALSE)\n\n\n\n\nCroisement avec les informations auxilaires sur les stations :\n\nstationcaract(station,clustering=classifkmeans)\n\n\n\n\nVisualisation d’une classe par exemple\n\nplotmapquali(velib$position,as.factor(classifkmeans==5))"
  },
  {
    "objectID": "TP5-Velib-Correction.html#clustering-avec-de-la-classification-hiérarchique",
    "href": "TP5-Velib-Correction.html#clustering-avec-de-la-classification-hiérarchique",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "2.2 Clustering avec de la classification hiérarchique",
    "text": "2.2 Clustering avec de la classification hiérarchique\nQuestion : A l’aide d’une procédure hiérarchique, déterminez une classification des stations.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nKmax&lt;-20\nresward&lt;-hclust(dist(velibacp,method=\"euclidean\"),method=\"ward.D2\")\ndf&lt;-data.frame(K=1:Kmax,height=sort(resward$height,decreasing=T)[1:Kmax])\nggplot(df,aes(x=K,y=height))+geom_line()+geom_point()\n\n\n\n\n\nCH&lt;-NULL\nKmax&lt;-20\nfor (k in 2:Kmax){\n  CH&lt;-c(CH,index.G1(velibacp,cutree(resward,k)))\n}\ndaux&lt;-data.frame(NbClust=2:Kmax,CH=CH)\nggplot(daux,aes(x=NbClust,y=CH))+geom_line()+geom_point()\n\n\n\n\n\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue par CAH. Comparez cette classification avec celle obtenue par les Kmeans.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\nLes critères précédents proposent de retenir 3 ou 5 classes :\n\nclassifCAH3&lt;-cutree(resward,3)\nclassifCAH5&lt;-cutree(resward,5)\ntable(classifCAH3,classifCAH5)\n\n           classifCAH5\nclassifCAH3   1   2   3   4   5\n          1 163   0   0   0 180\n          2   0 237 180   0   0\n          3   0   0   0 429   0\n\n\nOn visualise la classification sur la carte 2D :\n\nplotmapquali(velib$position,as.factor(classifCAH5))\n\n\n\n\n\nProfil moyen par classe :\n\nplotprofilsmoy(Velibdata,clustering=classifCAH5,plotsem=TRUE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=classifCAH5,plotsem=FALSE)\n\n\n\n\nCroisement avec les informations auxilaires sur les stations :\n\nstationcaract(station,clustering=classifCAH5)\n\n\n\n\nVisualisation d’une classe par exemple\n\nplotmapquali(velib$position,as.factor(classifCAH5==2))\n\n\n\n\n\nComparaison avec la classification des Kmeans :\n\ntable(classifkmeans,classifCAH5)\n\n             classifCAH5\nclassifkmeans   1   2   3   4   5\n            1   0  22   3 381   0\n            2   0 119  26   0   6\n            3  12  37   0   0 160\n            4 151  33   0  43  14\n            5   0  26 151   5   0\n\nadjustedRandIndex(classifkmeans,classifCAH5)\n\n[1] 0.6411121"
  },
  {
    "objectID": "TP5-Velib-Correction.html#clustering-avec-des-mélanges-gaussiens",
    "href": "TP5-Velib-Correction.html#clustering-avec-des-mélanges-gaussiens",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "2.3 Clustering avec des mélanges gaussiens",
    "text": "2.3 Clustering avec des mélanges gaussiens\nQuestion : A l’aide des mélanges gaussiens, déterminez une classification des stations.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nSélection du meilleur modèle de mélange par le critère ICL :\n\n\nset.seed(12345)\nresICLall&lt;-mclustICL(velibacp,G=2:20)\nsummary(resICLall)\n\nBest ICL values:\n             VEV,7       VVE,10        VVE,7\nICL      -40227.06 -40271.08062 -40281.27660\nICL diff      0.00    -44.02224    -54.21822\n\n\n\nresICL&lt;-Mclust(velibacp,G=7,modelNames = \"VEV\")\n\n\ndu meilleur modèle de mélange par le critère BIC\n\n\nresBICall&lt;-mclustBIC(velibacp,G=2:20)\nsummary(resBICall)\n\nBest BIC values:\n         VVE,10       VEI,13       VEE,13\nBIC      -39953 -39975.77232 -39981.70915\nBIC diff      0    -22.77722    -28.71404\n\nresBIC&lt;-Mclust(velibacp,G=10,modelNames = \"VVE\")\n\n\n\nQuestion : A l’aide des fonctions auxiliaires données dans la première partie, étudiez la classification obtenue par modèles de mélange. Comparez cette classification avec celles obtenues précédemment.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nAux&lt;-data.frame(label=paste(\"Cl\",resICL$classification,sep=\"\"), proba=apply(resICL$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resICL,data=velibacp,ellipse=F,geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\n\n\nplotprofilsmoy(Velibdata,clustering=resICL$classification,plotsem=TRUE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=resICL$classification,plotsem=FALSE)\n\n\n\n\n\nplotmapquali(velib$position,as.factor(resICL$classification))\n\n\n\n\n\n\ntable(resICL$classification)\n\n\n  1   2   3   4   5   6   7 \n331 186  83 207 132 128 122 \n\n\nComparaison entre la classification avec ICL et celle des Kmeans\n\ntable(resICL$classification,classifkmeans)\n\n   classifkmeans\n      1   2   3   4   5\n  1  42  89  79  89  32\n  2  44   0   0 142   0\n  3   2   0   0   0  81\n  4 190   0   0   8   9\n  5   0   0 130   2   0\n  6 128   0   0   0   0\n  7   0  62   0   0  60\n\nadjustedRandIndex(resICL$classification,classifkmeans)\n\n[1] 0.3285872"
  },
  {
    "objectID": "TP5-Velib-Correction.html#préparation-des-données-1",
    "href": "TP5-Velib-Correction.html#préparation-des-données-1",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "3.1 Préparation des données",
    "text": "3.1 Préparation des données\n\nVelibMean&lt;-matrix(0,nrow=nrow(Velibdata),ncol=3*7)\nfor (jour in 1:7){\n  VelibMean[,3*(jour-1)+1]&lt;-apply(Velibdata[,24*(jour-1)+c(1:8)],1,mean)\n  VelibMean[,3*(jour-1)+2]&lt;-apply(Velibdata[,24*(jour-1)+c(9:21)],1,mean)\n  VelibMean[,3*(jour-1)+3]&lt;-apply(Velibdata[,24*(jour-1)+c(22:24)],1,mean)\n}\ncolnames(VelibMean)&lt;-paste(rep(unique(day),each=3),\"-\",rep(c(\"0-7h\",\"8h-20h\",\"21-23h\"),7),sep=\"\")\n\nQuestion : Etudiez les corrélations entre variables du jeu de données VelibMean.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ncorrplot(cor(VelibMean[,((1:7)*3)-rep(c(2,1,0),each=7)]),method=\"ellipse\")\n\n\n\n\n\n\nQuestion : Faites une ACP et commentez.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nresacpMean&lt;-PCA(VelibMean,scale.unit = T,graph = F, ncp = 10)\n# Variance expliquée\nfviz_eig(resacpMean)\n\n\n\n# Visualisation des variables\n\nfviz_pca_var(resacpMean,axes=c(1,2),geom=c(\"arrow\",\"text\"),col.var=rep(levels(day),each=3))\n\n\n\nfviz_pca_var(resacpMean,axes=c(1,2),geom=c(\"arrow\",\"text\"),col.var=rep(c(\"0-7h\",\"8-20h\",\"21-23h\"),7))\n\n\n\ncorrplot(resacpMean$var$cor[,1:3],method=\"ellipse\")\n\n\n\n# Projection des individus\nfviz_pca_ind(resacpMean,axes=c(1,2),geom = c(\"point\"),habillage=as.factor(station$colline))\n\n\n\nfviz_pca_ind(resacpMean,axes=c(1,2),geom = c(\"point\"),habillage=as.factor(station$tourisme))\n\n\n\nfviz_pca_ind(resacpMean,axes=c(1,2),geom = c(\"point\"),habillage=as.factor(station$gare))"
  },
  {
    "objectID": "TP5-Velib-Correction.html#méthodes-de-classification",
    "href": "TP5-Velib-Correction.html#méthodes-de-classification",
    "title": "TP 5 - Clustering des données Vélib",
    "section": "3.2 Méthodes de classification",
    "text": "3.2 Méthodes de classification\n\n3.2.1 Kmeans\nQuestion : Faites une classification des stations à partir du jeu de données VelibMean à l’aide des Kmeans. Etudiez la classification obtenue.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nset.seed(12345)\nKmax&lt;-20\nreskmeansbis&lt;-matrix(0,nrow=nrow(VelibMean),ncol=(Kmax-1))\nIintra&lt;-NULL\nfor (k in 2:Kmax){\n  aux&lt;-kmeans(VelibMean,k)\n  reskmeansbis[,(k-1)]&lt;-aux$cluster\n  Iintra&lt;-c(Iintra,aux$tot.withinss)\n}\n\ndf&lt;-data.frame(K=2:Kmax,Iintra=Iintra)\nggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()\n\n\n\n\nEtude de la classification obtenue\n\nclassifkmeansbis&lt;-reskmeansbis[,4]\ntable(classifkmeansbis)\n\nclassifkmeansbis\n  1   2   3   4   5 \n174 254 200 253 308 \n\nplotprofilsmoy(Velibdata,clustering=classifkmeansbis,plotsem=FALSE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=classifkmeansbis,plotsem=TRUE)\n\n\n\nplotmapquali(velib$position,as.factor(classifkmeansbis))\n\n\n\n\n\nComparaison avec la classification obtenue sur les coordonnées de l’ACP :\n\ntable(classifkmeans,classifkmeansbis)\n\n             classifkmeansbis\nclassifkmeans   1   2   3   4   5\n            1   1   5 121   0 279\n            2   9  98   0  44   0\n            3   8   0   0 201   0\n            4 156   1  76   8   0\n            5   0 150   3   0  29\n\n\nComparaison avec les variables colline et tourisme\n\ntable(classifkmeansbis,station$colline)\n\n                \nclassifkmeansbis   0   1\n               1 172   2\n               2 253   1\n               3 149  51\n               4 252   1\n               5 236  72\n\ntable(classifkmeansbis,station$tourisme)\n\n                \nclassifkmeansbis   0   1\n               1 165   9\n               2 155  99\n               3 182  18\n               4 225  28\n               5 241  67\n\n\n\n\n\n\n3.2.2 Mélanges gaussiens\nQuestion : Faites une classification des stations à partir du jeu de données VelibMean à l’aide des mélanges gaussiens. Etudiez la classification obtenue.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nset.seed(12345)\nresICLallbis&lt;-mclustICL(VelibMean,G=2:20,modelNames=c(\"VVE\",\"VEV\",\"EVV\",\"VVV\"))\nsummary(resICLallbis)\n\nBest ICL values:\n           VVE,15     VVE,16    VVE,14\nICL      22181.01 22079.2176 22070.926\nICL diff     0.00  -101.7971  -110.089\n\n\n\nresICLbis&lt;-Mclust(VelibMean,G=15,modelNames = \"VVE\")\n\n\nAux&lt;-data.frame(label=paste(\"Cl\",resICLbis$classification,sep=\"\"), proba=apply(resICLbis$z,1,max))\nh1&lt;-ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\nh2&lt;-fviz_cluster(resICLbis,data=velibacp,ellipse=F,geom=\"point\")+ggtitle(\"\")+theme(legend.position = \"none\")\ngrid.arrange(h1,h2,ncol=2)\n\n\n\n\n\ntable(resICLbis$classification)\n\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \n 82 176  73  84  54 118  53  71  53  37  85  40 111  75  77 \n\nplotprofilsmoy(Velibdata,clustering=resICLbis$classification,plotsem=FALSE)\n\n\n\nplotprofilsmoy(Velibdata,clustering=resICLbis$classification,plotsem=TRUE)\n\n\n\nplotmapquali(velib$position,as.factor(resICLbis$classification))\n\n\n\n\nstationcaract(station,resICLbis$classification)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "4modIA-Clustering",
    "section": "",
    "text": "Cours de Clustering de 4modIA\n\n\n\nIntroduction\nDissimilarité, distance et inertie\nKmeans et variantes\nDBSCAN\nClassification hiérarchique\nModèles de mélange"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "TP3-CAH-Correction.html",
    "href": "TP3-CAH-Correction.html",
    "title": "TP 3 - CAH",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées pour classification hiérarchique. Les librairies R nécessaires pour ce TP :\nlibrary(mclust)\nlibrary(clusterSim)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(circlize)\nlibrary(viridis)\nlibrary(gridExtra)"
  },
  {
    "objectID": "TP3-CAH-Correction.html#lecture-des-données-et-fonctions-auxiliaires",
    "href": "TP3-CAH-Correction.html#lecture-des-données-et-fonctions-auxiliaires",
    "title": "TP 3 - CAH",
    "section": "2.1 Lecture des données et fonctions auxiliaires",
    "text": "2.1 Lecture des données et fonctions auxiliaires\nOn reprend dans cette partie le jeu de données zoo ainsi que les fonctions auxiliaires comme dans le TP1. On refait une analyse en composantes multiples pour la suite.\n\nzoo&lt;-read.table(\"zoo-dataTP.txt\",header=T,stringsAsFactors = TRUE)\nfor (j in 1:ncol(zoo))\n  zoo[,j]&lt;-as.factor(zoo[,j])\nsummary(zoo)\n\n hair   feathers eggs   milk   airbone aquatic predator toothed backbone\n 0:58   0:81     0:42   0:60   0:77    0:65    0:45     0:40    0:18    \n 1:43   1:20     1:59   1:41   1:24    1:36    1:56     1:61    1:83    \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n breathes venomous fins   legs   tail   domestic catsize\n 0:21     0:93     0:84   0:23   0:26   0:88     0:57   \n 1:80     1: 8     1:17   2:27   1:75   1:13     1:44   \n                          4:38                          \n                          5: 1                          \n                          6:10                          \n                          8: 2                          \n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\nres.mca&lt;- MCA(zoo,ncp = 5, graph = FALSE)\n\nfviz_screeplot(res.mca)\n\n\n\nfviz_mca_var(res.mca, col.var = \"darkblue\",repel=T)\n\n\n\nfviz_mca_ind(res.mca,repel=T)\n\n\n\n\nPour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction barplotClus() permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d’un clustering donné.\n\n# J indice des variables\n# Data = jeu de données\n# clust = clustering étudié\n# output : liste des graphes par variable dans J donnant la répartition des modalités de J par classe de clust\n\nbarplotClus &lt;- function(clust, Data, J) {\n    aux.long.p &lt;- heatm(clust, Data, J)$freq\n    p&lt;-NULL\n    \n    for (j in 1:length(J)) {\n        p[[j]] &lt;- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[J[j]]), ],\n            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = \"identity\")+\n            labs(fill = colnames(Data)[J[j]])\n    }\n    return(p)\n}\nheatm &lt;- function(clust, Data, J) {\n    library(dplyr)\n    Dataaux &lt;- data.frame(id.s = c(1:nrow(Data)), Data)\n    aux &lt;- cbind(Dataaux, clust)\n    aux.long &lt;- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,\n        id = c(\"id.s\", \"clust\"), factorsAsStrings = T)\n    # Effectifs\n    aux.long.q &lt;- aux.long %&gt;%\n        group_by(clust, variable, value) %&gt;%\n        mutate(count = n_distinct(id.s)) %&gt;%\n        distinct(clust, variable, value, count)\n    # avec fréquences\n    aux.long.p &lt;- aux.long.q %&gt;%\n        group_by(clust, variable) %&gt;%\n        mutate(perc = count/sum(count)) %&gt;%\n        arrange(clust)\n\n    Lev &lt;- NULL\n    for (j in 1:ncol(Data)) Lev &lt;- c(Lev, levels(Data[, j]))\n\n    Jaux &lt;- NULL\n    for (j in 1:length(J)) {\n        Jaux &lt;- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))\n    }\n\n    gaux &lt;- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +\n        scale_fill_gradient2(low = \"white\", mid = \"blue\", high = \"red\") + theme_minimal()\n\n    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))\n}"
  },
  {
    "objectID": "TP3-CAH-Correction.html#cah-directe-sur-variables-qualitatives",
    "href": "TP3-CAH-Correction.html#cah-directe-sur-variables-qualitatives",
    "title": "TP 3 - CAH",
    "section": "2.2 CAH directe sur variables qualitatives",
    "text": "2.2 CAH directe sur variables qualitatives\nQuestion : Quelle classification par CAH pouvez-vous mettre en place pour traiter des données qualitatives ? Mettez en application votre proposition avec R et étudiez la classification retenue. Vous pourrez vous aider des fonctions daisy() de la librairie cluster, de la fonction hclust(), …\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\ngower.dist &lt;-daisy(zoo,metric=c(\"gower\"))\naggl &lt;- hclust(gower.dist, method = \"complete\")\nggplot(data.frame(K=1:20,Height=sort(aggl$height,decreasing=T)[1:20]),aes(x=K,y=Height))+\n  geom_line()+\n  geom_point()\n\n\n\nfviz_dend(aggl,show_labels=TRUE,k=9)\n\n\n\n\n\nclustaggl&lt;-cutree(aggl,9)\nfviz_mca_ind(res.mca,geom.ind=c(\"point\",\"text\"),habillage=as.factor(clustaggl),repel=T)\n\n\n\ntable(clustaggl)\n\nclustaggl\n 1  2  3  4  5  6  7  8  9 \n36 13 15  7  5  6 10  5  4 \n\np&lt;-barplotClus(clust=clustaggl,zoo,J=c(1:4))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=clustaggl,zoo,J=c(5:8))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=clustaggl,zoo,J=c(9:12))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=clustaggl,zoo,J=c(13:16))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\n\n\nfviz_mca_var(MCA(data.frame(cl=as.factor(clustaggl),zoo),quali.sup=1,ncp = 5, graph = FALSE),col.var = \"darkblue\",repel=T)"
  },
  {
    "objectID": "TP3-CAH-Correction.html#cah-sur-les-coordonnées-de-mca",
    "href": "TP3-CAH-Correction.html#cah-sur-les-coordonnées-de-mca",
    "title": "TP 3 - CAH",
    "section": "2.3 CAH sur les coordonnées de MCA",
    "text": "2.3 CAH sur les coordonnées de MCA\nQuestion : Mettez en place une classification des animaux à partir des coordonnées de l’analyse en composantes multiples. Comparez avec la classification obtenue dans la section précédente.\n\n# A COMPLETER\n\n\n\n\n\n\n\nCorrection\n\n\n\n\nd&lt;-dist(res.mca$ind$coord,method=\"euclidean\")\nhward&lt;-hclust(d,method=\"ward.D2\")\nfviz_dend(hward,show_labels=TRUE)\n\n\n\ndaux1&lt;-data.frame(NbClust=1:Kmax,Intra=rev(hward$height[tail(1:nrow(zoo),Kmax)-1]))\nggplot(daux1,aes(x=NbClust,y=Intra))+geom_line()+geom_point()\n\n\n\nClustIntra&lt;-cutree(hward,4)\nfviz_dend(hward,show_labels=TRUE,k=4)\n\n\n\n\n\nfviz_mca_ind(res.mca,habillage=as.factor(ClustIntra),repel=T)\n\n\n\np&lt;-barplotClus(clust=ClustIntra,zoo,J=c(1:4))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=ClustIntra,zoo,J=c(5:8))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=ClustIntra,zoo,J=c(9:12))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\np&lt;-barplotClus(clust=ClustIntra,zoo,J=c(13:16))\ngrid.arrange(grobs=p,ncol=2)\n\n\n\nfviz_mca_var(MCA(data.frame(cl=as.factor(ClustIntra),zoo),quali.sup=1,ncp = 5, graph = FALSE),col.var = \"darkblue\",repel=T)\n\n\n\n\n\ntable(ClustIntra,clustaggl)\n\n          clustaggl\nClustIntra  1  2  3  4  5  6  7  8  9\n         1 36  0  0  0  0  0  0  0  0\n         2  0 13  0  1  5  1  0  5  3\n         3  0  0 15  0  0  5  0  0  0\n         4  0  0  0  6  0  0 10  0  1\n\nadjustedRandIndex(ClustIntra,clustaggl)\n\n[1] 0.7499927\n\nclust1F&lt;-paste(\"Cl-Direct\",clustaggl,sep=\"\")\nclust2F&lt;-paste(\"Cl-MCA\",ClustIntra,sep=\"\")\nchordDiagram(table(clust2F,clust1F))"
  },
  {
    "objectID": "TP4-MelangesGMM-SujetEtudiant.html",
    "href": "TP4-MelangesGMM-SujetEtudiant.html",
    "title": "TP 4 : Classification par modèles de mélanges",
    "section": "",
    "text": "L’objectif de ce TP est d’illustrer les notions abordées autour des modèles de mélanges.\nLes librairies R nécessaires pour ce TP :\nlibrary(mclust)\nlibrary(Rmixmod)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(reshape2)\n\nlibrary(circlize)\nlibrary(viridis)"
  },
  {
    "objectID": "TP4-MelangesGMM-SujetEtudiant.html#application-sur-données-simulées-uni-dimensionnelles",
    "href": "TP4-MelangesGMM-SujetEtudiant.html#application-sur-données-simulées-uni-dimensionnelles",
    "title": "TP 4 : Classification par modèles de mélanges",
    "section": "1.1 Application sur données simulées uni-dimensionnelles",
    "text": "1.1 Application sur données simulées uni-dimensionnelles\nQuestion : A l’aide du code suivant, simulez un jeu de données selon un mélange gaussien en \\(3\\) composantes unidimensionnel. Faites varier les différents paramètres (proportions, moyennes et variances).\n\nset.seed(1234)\na&lt;- ...\nb&lt;- ...\nmu&lt;-c(-a,0,a) # les moyennes \\mu_k\nsigma&lt;-c(b,0.5,b) # les \\sigma_k\nprop&lt;-c(0.2,0.3,0.5)\nn&lt;- ...\n\nZ&lt;-rmultinom(....)\n\nX&lt;-data.frame(X=c(rnorm(...),\n                  rnorm(...),\n                  rnorm(...)\nlabeltrue&lt;-    # vecteur des vrais labels\n\n\naux&lt;-seq(-(a+4),a+4,0.01)\nY&lt;-data.frame(x=aux,\n              y1=(prop[1]*dnorm(aux,mu[1],sigma[1])), \n              y2=(prop[2]*dnorm(aux,mu[2],sigma[2])),     \n              y3=(prop[3]*dnorm(aux,mu[3],sigma[3])))\n\ngvrai&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=Y,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=Y,col=\"blue\")+\n  geom_line(aes(x=x,y=y3),data=Y,col=\"green\")+\n  theme_minimal()\ngvrai\n\nQuestion : Estimez les paramètres d’un mélange à \\(K=3\\) classes à l’aide de la fonction Mclust() de la librairie mclust. Comparez la classification obtenue avec les vrais labels.\n\n# A completer\nres&lt;-Mclust(...)\ntable(...,...)\nadjustedRandIndex(...,...)\n\nQuestion : Représentez la densité de mélange estimée sur l’histogramme de l’échantillon simulé. Vous pouvez vous aider de la fonction dnorm() appliquée avec les différentes estimations de paramètres par composante.\n\n# A completer\n# dans y_k &lt;- \\pi_k \\times \\phi(x; \\mu_k,\\sigma_k^2)\n\nMelEstim&lt;-data.frame(x=aux,\n                     y1=....., \n                     y2=.....,\n                     y3=......)\nMelEstim&lt;-data.frame(MelEstim,Somme=apply(MelEstim[,2:4],1,sum))\n\ngMelEst&lt;-ggplot(X,aes(x=X))+\n  geom_histogram(aes(y = after_stat(density)),bins=100)+\n  geom_line(aes(x=x,y=y1),data=MelEstim,col=\"red\")+\n  geom_line(aes(x=x,y=y2),data=MelEstim,col=\"green4\")+\n  geom_line(aes(x=x,y=y3),data=MelEstim,col=\"blue\")+\n  geom_line(aes(x=x,y=Somme),data=MelEstim,col=\"yellow\",linetype = \"dashed\",linewidth=1.5)\ngMelEst\n\nQuestion : Calculez les probabilités a posteriori d’appartenance des individus à chacune des trois classes et tracez-les graphiquement.\n\n# dans p mettre le vecteur des proba a posteriori d'appartenance (t_{11},\\ldots,t_{n1},t_{12},\\ldots,t_{n3})\n\nMelProba&lt;-data.frame(x=rep(aux,3),\n                     p= c(..., ..., ..),    \n                     class=as.factor(rep(c(1,2,3),each=length(aux))))\n\ngprobapost&lt;-ggplot(MelProba,aes(x=x,y=p,col=class))+geom_line()\n\ngprobapost\n\nQuestion : Tracez les boxplots des probabilités d’appartenance maximales par classe. Vous pouvez vous aider de la fonction apply().\n\ndf&lt;-data.frame(lab=...,probamax=...)\ngprobamax&lt;-ggplot(df,aes(x=lab,y=probamax))+geom_boxplot()\ngrid.arrange(gvrai,gMelEst,gprobapost,gprobamax,ncol=2)"
  },
  {
    "objectID": "TP4-MelangesGMM-SujetEtudiant.html#application-sur-des-données-simulées-dans-mathbbr2",
    "href": "TP4-MelangesGMM-SujetEtudiant.html#application-sur-des-données-simulées-dans-mathbbr2",
    "title": "TP 4 : Classification par modèles de mélanges",
    "section": "1.2 Application sur des données simulées dans \\(\\mathbb{R}^2\\)",
    "text": "1.2 Application sur des données simulées dans \\(\\mathbb{R}^2\\)\nOn va ici utiliser les données simulées “ex4.1” disponibles dans la librairie mclust. Ces données sont simulées selon un mélange de densités gaussiennes, proposées dans Baudry et al (2010). L’objectif est d’étudier l’impact du choix des formes des mélanges considérées et de la différence d’objectif entre les critères BIC et ICL. On va au travers de ce jeu de données simulées simple appréhender la manipulation des fonctions pour le clustering par mélanges gaussiens avec R.\nOn commence ici par charger les données :\n\nlibrary(mclust)\ndata(Baudry_etal_2010_JCGS_examples)\nData&lt;-ex4.1\nggplot(Data,aes(x=X1,y=X2))+geom_point()\n\n\n\n\n\n1.2.1 Mélanges gaussiens diagonaux\nDans cette section, on va considérer une collection de modèles de mélanges gaussiens avec un nombre de composantes \\(K\\) variant entre \\(2\\) et \\(10\\) et des matrices de variance-covariance diagonales.\nQuestion : A l’aide de la fonction Mclust(), estimez les paramètres des mélanges gaussiens considérés. Vous pouvez consulter l’aide de la fonction mclustModelNames() pour le choix des formes des mélanges.\n\n# A COMPLETER\nresBICdiag&lt;-Mclust(....)\n\nQuestion : A l’aide de la fonction fviz_mclust_bic(), visualisez le comportement du critère BIC sur la collection de modèles. Quel modèle est sélectionné ? Contrôlez à l’aide de summary(resBICdiag).\n\nfviz_mclust(....,what=....)\nsummary(resBICdiag)\n\nQuestion : Tracez la classification obtenue sur le nuage de points (vous pouvez utiliser la fonction fviz_cluster()). Comment est obtenue cette classification à partir du mélange gaussien retenu ? Quels sont les effectifs par classe ? Contrôlez les probabilités a posterori d’appartenance.\n\n# Visualisation du clustering\nfviz_cluster(...)\n# Effectifs par classe\ntable(....)\n# Boxplot des probabilités a posteriori maximales\nAux&lt;-data.frame(label=...,    proba=...)\nggplot(Aux,aes(x=label,y=proba))+geom_boxplot()\n\nQuestion : Quel mélange gaussien est retenu avec le critère ICL ? Vous utiliserez la fonction mclustICL(). Etudiez la classification alors déduite.\n\nresICL&lt;-mclustICL(...)\nsummary(resICL)\n\n\n\n1.2.2 Toutes les formes de mélanges gaussiens\nQuestion : Reprenez les questions de la section précédente en considérant ici toutes les formes de mélanges gaussiens. Commentez.\n\n# A COMPLETER"
  },
  {
    "objectID": "TP4-MelangesGMM-SujetEtudiant.html#etude-des-données-de-vins",
    "href": "TP4-MelangesGMM-SujetEtudiant.html#etude-des-données-de-vins",
    "title": "TP 4 : Classification par modèles de mélanges",
    "section": "1.3 Etude des données de vins",
    "text": "1.3 Etude des données de vins\nOn reprend dans ce TP les données wine disponibles sur la page moodle du cours. On charge ici les données.\n\nwine&lt;-read.table(\"wine.txt\",header=T)\nwine$Qualite = as.factor(wine$Qualite)\nwine$Type = factor(wine$Type, labels = c(\"blanc\", \"rouge\"))\n\nwineinit&lt;-wine\nwine[,-c(1,2)]&lt;-scale(wine[,-c(1,2)],center=T,scale=T)\n\nhead(wine)\n\n     Qualite  Type      AcidVol    AcidCitr     SO2lbr      SO2tot     Densite\n1352  medium rouge  1.638714588 -1.92626362 -1.2083376 -1.15967786 -0.46497450\n5493  medium blanc -0.068544417 -1.35617574 -0.7004747 -0.85707581 -0.33499781\n5153  medium blanc -0.800226847 -0.59605856  0.5409681 -0.02047014  1.32391517\n5308  medium blanc -0.007570881  0.92417581  1.7824108  1.27893867  1.08790487\n3866  medium blanc  0.419243870  0.03737243 -0.5311870  0.99413674  0.03783006\n694   medium rouge  0.785085086  0.03737243 -0.4747578  0.19313131  1.27260858\n          Alcool\n1352  1.14546909\n5493 -1.12092616\n5153 -1.29526426\n5308 -1.29526426\n3866  0.09944051\n694  -0.94658806\n\n\nOn fait une ACP pour la visualisation des résultats dans la suite\n\nresacp&lt;-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)\n\nQuestion : Déterminez une classification de ces données à l’aide d’un modèle de mélange. Comparez votre résultat avec les classifications obtenues dans les TP précédents (avec Kmeans, CAH).\n\n# A FAIRE"
  }
]